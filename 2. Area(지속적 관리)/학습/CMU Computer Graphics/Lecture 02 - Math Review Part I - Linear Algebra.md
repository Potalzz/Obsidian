>This post is a personal study note based on CMU 15-462 (Computer Graphics) lectures.
>All original lecture slides and videos are copyrighted by the instructors.

강의에 대한 정보와 자료는 아래 홈페이지에서 확인할 수 있습니다.
https://15462.courses.cs.cmu.edu/fall2020/home

본 포스팅은 강의 내용을 바탕으로 하되, 이해를 돕기 위해 별도의 자료 조사와 개념 정리를 덧붙여 작성했습니다. 따라서 원 강의 내용과 100% 일치하지 않을 수 있으며, 개인적인 학습 내용이 포함되어 있습니다.

---

### 컴퓨터 그래픽스에서 선형 대수학은 왜 중요한가?

선형대수학은 그래픽 분야에서 다양한 문제를 공식화하기 위한 매우 강력한 추상화이다.

선형 대수학은 벡터 공간 간의 선형 변환 대한 연구이다. 
그래픽스의 많은 분야에서, 문제를 선형 대수학으로 표현할 수 있다면 거의 해결된 것이나 다름없다. 컴퓨터에게 $Ax=b$를 풀라고 요청하기만 하면 된다.

벡터에 대한 정의와 기본적인 연산은 이전에 포스팅에서 정리했었.
[공간 컴퓨팅에서의 수학(벡터)](https://medium.com/@tprhkd1607/visionos%EC%97%90%EC%84%9C%EB%8A%94-%EC%88%98%ED%95%99%EC%9D%B4-%EC%96%B4%EB%96%BB%EA%B2%8C-%ED%99%9C%EC%9A%A9%EB%90%A0%EA%B9%8C-7723c660d033)

벡터가 그래픽스에서 어떻게 활용되는지 이번 강의를 통해 더욱 깊게 알아보고자 한다.

벡터 공간은 모든 벡터 `u, v, w`와 스칼라에 대해 아래의 방식으로  동작한다.

- $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$
- $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$
- There exists a _zero vector_ "$\mathbf{0}$" such that $\mathbf{v} + \mathbf{0} = \mathbf{0} + \mathbf{v} = \mathbf{v}$
- For every $\mathbf{v}$ there is a vector "$-\mathbf{v}$" such that $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$
- $1\mathbf{v} = \mathbf{v}$
- $a(b\mathbf{v}) = (ab)\mathbf{v}$
- $a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$
- $(a + b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$

하지만 위 공식들은 강의에서 원하는 시작 방식이 아니다.
벡터 공간에 대해 말하고 싶은 모든 것들은 아주 자연스러운 곳에서 유래했다.
이것은 연구하기 매우 자연스러운 대상이며, 단순히 추상적인 수학적 대상이 아니다.

일련의 규칙 말고, 다른 관점에서 살표보자.
- **이 규칙들이 어디에서 유래되었는가 ?**
- **왜 유용하고 의미있는 것인가 ?**

> **단순히 정의가 무엇인지뿐만이 아니라, 왜 그 내용이 사실인지에 대해 생각해보자.**

**벡터에 대해 접해본 적이 없는 사람에게 벡터란 무엇인지 어떻게 설명할 것인가 ?**
아마 작은 화살표를 떠올릴 것이다.

**벡터는 어떤 정보를 담고 있는가 ?**

기본적으로 벡터는 **방향**과 **크기**를 담고있다.
이를 2차원에서 encode해보자면, 아래와 같이 표현할 수 있다.
*\*encode(벡터라는 추상적 대상을 특정 좌표계에 따라 숫자 형태로 표현)

![[Pasted image 20260122172125.png]]
크기는 길이 또는 반지름 r로 나타내고,
방향은 수평면과의 각도 $\theta$로 표현한다.

이걸 **극 좌표(polar coordinates)** 라고 부른다.(우리가 가장 흔히 보는 데카트르 좌표계와는 다르다)
극 좌표계에서는 r과 극축과 이루는 각 $\theta$의 순서쌍 $(r,\theta)$으로 나타난다.

여기서 의문이 들 수 있다.

**그래서 벡터는 어느 지점에서 뻗어 나오는건데 ?**

 선형대수에서 이야기할 때는 벡터는 서로 다른 기준점을 가지는 것이 아니라 모두 같은 공간의 **원점**을 기준으로 한다.
`(전통적으로, 벡터는 기준점을 포함하지 않는다. 기준점을 포함하는 벡터는 때때로 "tangent vector"라고 불린다.)`

위에서 얻은 데이터 r과 $\theta$를 사용할 때 유의해야 할 부분은, 하나의 좌표계에서 얻은 데이터를 **절대로 다른 좌표계에 적용해서는 안된다.**
이는 컴퓨터 그래픽에서 수 많은 버그를 유발하는 원인이며, (r, $\theta$)를 (x, y)로 변환하는 일련의 과정을 거쳐야한다.

극 좌표계에서의 값을 데카르트 좌표계에 적용하면 다른 벡터를 얻게 되므로 주의하자.

**데카르트 좌표계에서 벡터를 포현하면 아래와 같이 표현할 수 있다.**
![[Pasted image 20260122174137.png]]


**벡터가 무엇인지에 대해 알아봤으니, 벡터로 실제로 무엇을 할 수 있을까 ?**

기본적으로 두 가지 중류의 작업이 있다.

**1 - Addition**
![[Pasted image 20260122174423.png]]
위의 그림에서 보듯이, $u + v$의 의미는 u 방향으로 가다가 v방향으로 가라는 의미인데, 덧셈의 순서와 상관 없이 같은 결과가 나오므로 벡터의 덧셈은 **교환 법칙이 성립**된다는 사실을 알 수 있다.

$u + v = v + u$

**2 - Scaling**
![[Pasted image 20260122174839.png]]
벡터 u에 대해 스칼라 a배를 하고, 스칼라 b배를 해주던지, 스칼라 ab를 먼저 곱하고 벡터 u에 곱해주던지 결과가 같다는 사실을 알 수 있다.

$a(bu) = (ab)u$

벡터의 덧셈과 곱셉 두 가지에 대해 알아보았으니,
두 가지를 함께 적용해보자.
![[Pasted image 20260122175126.png]]
좌측과 같이 벡터를 먼저 더하고, 스칼라 곱을 적용하던,
우측과 같이 각각의 벡터에 스칼라 곱을 적용하고 두 벡터를 더하던 같은 결과가 나온다.

$a(u + v) = au + av$


이러한 기하학적 접근을 통해 위에서 나온 **벡터의 규칙**이 어떻게 성립되는지 이해할 수 있다.

- $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$
- $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$
- There exists a _zero vector_ "$\mathbf{0}$" such that $\mathbf{v} + \mathbf{0} = \mathbf{0} + \mathbf{v} = \mathbf{v}$
- For every $\mathbf{v}$ there is a vector "$-\mathbf{v}$" such that $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$
- $1\mathbf{v} = \mathbf{v}$
- $a(b\mathbf{v}) = (ab)\mathbf{v}$
- $a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$
- $(a + b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$


이러한 모든 속성을 만족하는 객체들의 집합을 **벡터 공간**이라고 부른다.

이러한 사소한 속성들을 그림을 그려 나가면서 이해하는 과정은 중요하다.
복잡한 규칙이나 물체가 나오면 **왜 저 규칙들이 맞는 걸까?** 라고 의문을 갖고 이해해나가는 습관을 가지자.

**벡터로서의 함수**
위에서 살펴 본 화살표와 함수는 똑같이 동작한다.

![[Pasted image 20260122180645.png]]
각각의 x에 대해서 f를 평가하고, g를 평가하여 결과를 취한다.

시각적으로 화살표처럼 보이지 않더라도 분명히 **함수는 벡터이다.**

**From Geometry to Algebra**

앞의 과정에서 기하학적인 접근을 통해 대수적인 규칙을 증명할 수 있었다.
$u + v = v + u$ 해당 규칙을 간단히 살펴보자면 아래와 같이 확인할 수 있었다.

$$\begin{align} u + v = (u_1, u_2) + (v_1 + v_2) = (u_1 + v_1,u_2 + v_2) \\=(v_1 + u_1, v_2 + u_2) =(v_1, v_2) + (u_1, u_2) = v + u \end{align}$$

이처럼 실제 규칙을 맹목적으로 받아들이는 것이 아니라, 항상 이 규칙은 어디서 왔고 기하학적으로 무엇을 의미하는지, 그림을 그려볼 수 있는지 의문을 가져야한다.

**Computing the Midpoint**

![[Pasted image 20260123121402.png]]
벡터 a , b의 중간 지점을 어떻게 계산할 수 있을까 ?

$m = \cfrac {1}{2}(a+b)$
- 중간 지점 m은 a, b합의 절반에 위치

$= \cfrac {1}{2}((3, 4) + (7, 2))$
- a, b를 각각 좌표로 변환

$=\cfrac {1}{2}(10,6)$
- 스칼라 곱셈은 덧셈에 대해 분배 법칙이 성립하므로 각 좌표를 먼저 더해줌

$=(5,3)$
- 결과 도출

초반에 벡터는 방향과 크기를 포함하고 있다고 하였다. 벡터가 주어졌을 때 r과 $\theta$를 어떻게 구할 수 있을까 ?

### Norm of a Vector

벡터 v가 주어졌을 때 벡터의 크기 |v| 값을 구해보자.
(여기서 |v|는 길이, 크기, 노름(norm) 모두 동일한 의미를 가진다.)

**함수의 노름이란 단어는 무엇을 의미할까 ?**
![[Pasted image 20260123123111.png]]

직관적으로 보았을 때 좌측 벡터가 더 작은 노름 값을 가진다는 것을 알 수 있다.

**노름의 정의**
> 노름은 벡터 공간의 각 벡터에 숫자를 할당하는 모든 함수이다.

노름은 모든 벡터 u,v와 모든 스칼라 a에 대해 다음 속성을 만족한다.
**$|v| >= 0$**
- **길이는 마이너스가 될 수 없다.**

**$|u| = 0 <=> u  = 0$**
- **길이가 0인 것은 원점(영벡터)밖에 없다.**

**$|av| = |a||v|$**
- **벡터를 늘리면, 길이도 그만큼(절댓값만큼) 늘어난다.**ㅏ

**$|u| + |v| >= |u+v|$**
- **직선 거리는 돌아가는 길보다 짧거나 같다.**
- $|u+v|$는 시작점에서 도착점으로 바로 가는 **직선거리**
- $|u| + |v|$는 중간 지점을 거쳐서 **돌아가는 거리**

강의에서 각 규칙에 대해 기하학적으로 왜 그런지 이해할 수 있다.

**유클리드 노름**
n차원 벡터의 대한 표준 노름은 **유클리드 노름(Euclidean norm) 이라고 부른다.

$$\lvert \mathbf{u} \rvert = \lvert (u_1, \dots, u_n) \rvert := \sqrt{\sum_{i=1}^{n} u_i^2}$$
[유클리드 노름의 수식]

**수식을 하나씩 풀어보면**

- $\mathbf{u} = (u_1, u_2, \dots, u_n)$
    → n차원 벡터
    
- $|\mathbf{u}|$
    → 벡터 $\mathbf{u}$의 **길이(length, magnitude)**
    
- $\sum_{i=1}^{n} u_i^2$
    → 각 성분을 **제곱해서 전부 더함**
    
- $\sqrt{\;\;}$
    → 그 합에 **제곱근**

  

즉,
> **“각 좌표를 제곱해서 더한 뒤, 루트를 씌운 값” = 벡터의 길이**

![[Pasted image 20260123130944.png]]


### L2 Norm of Functions
함수의 가장 기본적인 노름 중 하나는 L2노름.
- L2노름은 함수의 전체 크기를 측정한다.

단위 구간$[0, 1]$에서 제곱 적분 가능한 함수들에만 초점을 맞춰보자.
L2노름은 다음과 같이 정의된다.
$$\|f\| := \sqrt{\int_{0}^{1} f(x)^2 \, dx}$$
 위에서 정의한 유클리드 노름과 개념적으로 크게 다르지 않다.
 단순히 합계를 적분으로 대체했을 뿐이다.

**f(x) := x$\sqrt 3$함수에 대하여 해당 그래프의 $L^2$노름 값을 구해보자.**
![[Pasted image 20260123142614.png]]

수식에 숫자를 대입하여 적분을 진행하게 되면 아래와 같은 결과가 나온다.
$$
\|f\|^2 = \int_{0}^{1} 3x^2\,dx = [x^3]_0^1 = 1^3 - 0^3= 1
$$

$x\sqrt3$은 단위 노름 함수, 즉 단위 길이 벡터이다.
하지만 실제 그래픽에서 적분은 대부분 이런 방식으로 계산되지 않는다.
훨씬 복잡할 것이고 직접 계산하기보다 컴퓨터에게 계산을 맡기게 될 것이다.

### Inner Product(내적)
> 내적은 두 벡터 간의 유사도를 나타낸다.

수업에서는 내적에 대해 다음과 같은 표기법을 사용할 것이다.
$$<u,v> (u \cdot v)로 \space 표기하기도 \space 함.$$

**내적의 대칭성**
내적의 기본적인 속성 중 하나는 대칭성이다.
u와 v가 정렬된 정도와 v가 u와 정렬된 정도는 같으므로 순서는 중요하지 않다.
$$<u,v> \space = \space <v, u>$$
내적이 대칭한다고 하니 어렵게 들릴 수 있지만, 그냥 u가 바라보는 u와 v의 각도나, v가 바라보는 v와 u의 각도가 동일하다고 이해하면 쉽다.

**내적의 투영(Projection) 및 스케일링**
다만 유의해야 할 점은, 내적은 벡터의 길이에 따라 영향을 받는다는 점이다.
벡터 중 하나를 스케일링하면 내적도 스케일링된다.

$$<2v, u> \space = \space 2<v,u>$$

순수하게 방향(취향)만 비교하기 위해 내적 값을 앞서 측정한 각 벡터의 크기로 나누어 주는데, 이것을 코사인 유사도 (Cosine Similarity)라고 한다.

**내적의 형식적 정의**
내적은 두 벡터 u,v에 숫자 $<u, v>$를 할당하고 다음 속성을 만족하는 함수이다.
- $\langle u, v \rangle = \langle v, u \rangle$
- $\langle u, u \rangle \ge 0$
- $\langle u, u \rangle = 0 \iff u=0$
- $\langle au, v \rangle = a\langle u, v \rangle$
- $\langle u+v, w \rangle = \langle u, w \rangle + \langle v, w \rangle$

![[Pasted image 20260124171149.png]]
강의에서 위 4가지에 대해서는 기하학적으로 살펴 보았지만, 마지막 속성에 대해서는 왜 그럽게 성립되는지에 대한 질문을 던진다.

**직접 그림을 그려서 $\langle u+v, w \rangle = \langle u, w \rangle + \langle v, w \rangle$가 왜 성립되는지 살펴보자.**

기하학적으로 내적의 값은 **'상대방 벡터 위로 비친 그림자의 길이'**에 **'그림자가 비치는 바닥(기준) 벡터의 길이'**를 **곱한 값**이다.

계산을 용이하게 하기 위해서 **벡터 w는 지면**, **벡터 u, v, (u + v)는 막대기**라고 가정하고 그림을 그려보자.

![[Pasted image 20260123161917.png]]

벡터 w에 수직인 방향에서 빛이 비춰진다고 하면,
각각의 막대기에 위와 같이 그림자가 생길 것이다.

이때 내적 $\langle u, w \rangle$의 값은 **(막대기 $u$의 그림자 길이) $\times$ (지면 $w$의 길이)** 가 된다.
이전에 벡터의 크기가 커지면 내적의 값이 커진다고 한 이유는, 막대기가 길어지면 그림자도 길어지고, 지면이 길어지면 곱해지는 값도 커지기 때문이다.


![[Pasted image 20260123162254.png]]
그림을 보면 **$(u+v)$의 그림자 길이**는 **$u$의 그림자 길이**와 **$v$의 그림자 길이**를 합친 것과 정확히 같다.
여기에 공통적으로 **지면($w$)의 길이**만 곱해주면 되므로,
최종적으로 $\langle u+v, w \rangle = \langle u, w \rangle + \langle v, w \rangle$라는 결과가 성립함을 알 수 있다.

**Inner Product in Cartesian Coordinates**
표준 내적은 소위 **유클리드 내적**으로, n-벡터 쌍에 대해 다음과 같이 계산한다.
$$\langle u, v \rangle := \sum_{i=1}^{n} u_i v_i$$
위 그림에 값을 대입해 계산해보면 같은 결과가 나온다는 것을 알 수 있다.

![[Pasted image 20260123163906.png]]


**L2 Inner Project of Functions**
함수의 노름처럼, 두 함수가 얼마나 잘 정렬되어 있는지 측정하는 내적도 정의할 수 있다.

![[Pasted image 20260123164301.png]]


**Measuring Images, Other Signals ?**
노름이나 내적의 값을 측정은 많은 방법이 있다.

응용 분야에 따라 내적의 선택이 달라진다.

![[Pasted image 20260123165211.png]]
미분의 노름을 측정하여 위와 같이 흥미로운 이미지를 찾는 알고리즘을 만들 수도 있다.

### Linear Maps
>**원점을 고정한 채로, 공간을 “찌그러뜨리거나 늘리거나 회전시키는” 변환**

선형 사상/변환은 쉽게 말해 **함수**다. 단, **선형성**을 만족하는.

**선형 변환이 왜 그래픽스에 유용한가 ?**
- 계산적으로 선형 방적식을 풀기가 쉽다
- 기본 변환(회전, 이동, 크기 조절 등)을 선형 변환으로 표현 가능
- 모든 사상은 짧은 거리/시간 동안 선형 변환으로 근사될 수 있다. (테일러 정리)

 ![[Pasted image 20260124172551.png]]
 - 원점은 원점으로 이동해야 한다.
 - 직선은 직선으로 맵핑되어야 함.

두 벡터 공간 V, W에 대해

함수 f: $U \rightarrow V$ 가 **linear map** 이려면 아래 **두 조건을 모두** 만족해야 한다.

**덧셈 보존**
- $f(u+v) = f(u) + f(v)$

**스칼라 곱 보존**
- $f(au) = a f(u)$

![[Pasted image 20260124173337.png]]


**Linear Maps in Coordinates**
$\mathbb{R}^m$에서 $\mathbb{R}^n$으로 가는 선형 사상의 경우 더 명확한 정의가 가능하다.
	ex) 2차원 -> 3차원으로의 선형 변환

벡터 $u$에 대해서 선형 사상 $f$를 적용했을 때 $u$의 성분들에 대한 합으로 나타낼 수 있다면 선형이다.
$$
f(u_1, ..., u_m) = \sum_{i=1}^{m} u_i a_i
$$
즉, 고정된 벡터 집합 $a_i$들의 선형 결합(linear combination)인 경우이다.
**입력 벡터의 각 좌표** $u_i$를 계수로 해서, 미리 정해진 벡터 $a_i$들을 섞어서 **출력 벡터를 만든다**는 의미.

![[Pasted image 20260124183215.png]]
강의에 나온 이미지를 보고, 이 그림이 왜 선형 사상에 만족하는지 차근차근 살펴보자.

선형 사상(Linear Maps)는 선형성을 만족하는 **함수**라고 하였다.

위 그림은 2차원에서 3차원으로 선형 변환을 하는 상황이다.
- 입력 공간: $\mathbb{R}^2$
- 출력 공간: $\mathbb{R}^3$

즉,
$f : \mathbb{R}^2 \rightarrow \mathbb{R}^3$
**2차원 실수 벡터 공간의 각 벡터를, 규칙에 따라 3차원 실수 벡터 공간의 한 벡터에 대응시키는 함수(선형 사상).**

$u$는 2차원의 입력 벡터이므로 아래와 같이 생겼다.
$$\mathbf{u} = (u_1, u_2)$$
2차원 → 3차원 선형 사상이므로,
$a_1$, $a_2$는 **3차원 공간에 있는 벡터**이며
입력 공간의 표준 기저 $e_1$, $e_2$가 선형 사상 f에 의해 어디로 이동했는지를 나타낸다.

$a$의 좌표는 대략 아래와 같을 것이다.
$$\mathbf{a}_1 = (3,1,0),\; \mathbf{a}_2 = (0,1,3)$$

선형 사상은 아래와 같으므로,
$$
f(u_1,u_2) = u_1 a_1 + u_2 a_2
$$

계산 결과는 다음과 같이 나온다.
$$
f(u_1,u_2) = (3u_1,\; u_1,\; 0) + (0,\; u_2,\; 3u_2)
= (3u_1,\; u_1 + u_2,\; 3u_2)
$$
 
**왜 선형이면 위 과정이 맞다고 생각할 수 있을까 ?**

왜냐하면
- 모든 입력 벡터는 **좌표들의 선형 결합**
    
- 출력도 그 좌표들로 **고정 벡터들을 선형 결합**

즉,  
> “입력 좌표를 그대로 가중치로 써서 출력 벡터를 만든다”

이 구조 자체가 **선형성의 정의**와 완전히 동일하다.

결과적으로 **선형 사상에 의해 얻어지는 출력 벡터는** 고정된 벡터 집합 $a_i$들의 선형 결합으로 표현된다.

### Linear vs Affine Maps
![[Pasted image 20260125144529.png]]


$f(x) = ax + b$와 같은 아핀 함수는 원점을 통과하지 않기 때문에 선형 함수가 아니다.
함수에 0을 대입해도 0이 나오지 않기 때문에 작업 내용을 보존하지 못한다.

![[Pasted image 20260125144714.png]]
아핀 함수는 선형성 조건 중 가산성(합쳐서 처리하나, 따로 처리해서 합치나 결과가 같음)을 만족하지 못하므로 선형 함수라고 볼 수 없다.

나중에 아핀 함수를 선형적으로 변환하는데 적용할 수 있는 방법을 배울 것이고,
이는 그래픽 파이프라인을 설계하는데 중요한 요소이다.


### Span

스팬은 $u$와 $v$의 선형 결합으로 쓸 수 있는 모든 벡터들의 집합.
즉, $au + bv$ 형태의 벡터들을 의미한다.

$$span(u_1, ..., u_k) = \{ x \in V | x = \sum a_i u_i \}$$
[일반적인 정의]

$$f(u) = u_1 a_1 + u_2 a_2$$
[모든 선형 사상의 **이미지**는 어떤 벡터들의 모음의 스팬이다.]
함수에서의 사진과 같은 **이미지**를 의미하는 것이 아니라, **도달할 수 있는 모든 점**을 이미한다.

### Basis
>기저는 벡터 공간을 구성하는 **최소한의 필수 재료(벡터)들의 집합**을 의미한다.

스팬은 기저(Basis)의 개념과 밀접하게 관련되어 있다.

수학적으로 어떤 벡터들의 집합이 기저가 되려면, 다음 **두 가지 조건**을 반드시 동시에 만족해야 한다.

1. **생성(Span) 가능해야 한다.**
    - 이 벡터들을 조합(상수배 하고 더하기)해서 그 공간에 있는 **모든** 벡터를 만들 수 있어야 한다.
        
2. **선형 독립(Linearly Independent)이어야 한다.**
    - 벡터들끼리 겹치는 방향이 없어야 한다. 즉, 재료 중에 불필요한(다른 재료로 만들 수 있는) 중복이 없어야 한다는 뜻이다.

만약 $span(e_1, ..., e_n) = \mathbb{R}^n$을 만족하는 정확히 $n$개의 벡터가 있다면,
이 벡터들을 $\mathbb{R}^n$의 **기저**라고 한다.
풀어서 얘기하면, 재료인 $e_1$부터 $e_n$까지의 벡터들을 선형 결합(조합)하면, $n$차원 공간($\mathbb{R}^n$)에 있는 **그 어떤 벡터라도 빠짐없이 다 만들어낼 수 있다**는 것을 의미한다.

### 정규 직교 기저(Orthonormal Basis)
>길이가 1이고, 서로 직교하는 기저 벡터들.

![[Pasted image 20260123183321.png]]
정규 직교 기저는 위 두 가지 조건을 만족해야 한다.
- **조건 1: $i = j$ 일 때 (자기 자신과의 내적) $\rightarrow$ 1**
    - $\langle \mathbf{e}_1, \mathbf{e}_1 \rangle = 1$
    - **이 축의 길이는 정확히 1이다.**
        
- **조건 2: $i \neq j$ 일 때 (남과의 내적) $\rightarrow$ 0**
    - $\langle \mathbf{e}_1, \mathbf{e}_2 \rangle = 0$
    - **X축과 Y축은 서로 직각(90도)이다.** (내적이 0이면 수직)

**이게 우리가 사용하는 좌표계의 근간이 된다.**
길이가 1이고 두 축이 서로 직각을 이루기 때문에, 우리가 숫자를 입력하면 컴퓨터가 스칼라 곱을 통해서 해당 좌표로 이동할 수 있게 된다.

**만약 정규 직교 기저가 없다면 ?**
비정규 기저에서 작업을 하면 수 많은 버그의 원인이 된다.

만약 x축의 단위가 2라고 가정해보자.
2,3으로 이동하기 위해 x축에 2 스칼라 곱을 했는데, 단위가 2이기 때문에 실제 도착하는 좌표는 4,3이 될 것이다.
이처럼 비정규 기저에서의 작업은 우리가 흔히 아는 좌표계가 아니기 때문에 좌표의 도착 위치가 예상한대로 동작하지 않을 것이다.

**그렇다면 정규 직교 기저를 어떻게 찾을 수 있을까 ?**

### Gram-Schmidt
> 기저 벡터 모음이 주어졌을 때, 정규 직교 기저 벡터를 얻는 알고리즘

![[Pasted image 20260125150823.png]]
벡터 $u_1$과 $u_2$가 주어짐.
1. 벡터 하나($u_1$) 가져와서 축으로 설정하고 정규화를 통해 길이를 1로 설정. = $e_1$
2. $e_1$ 벡터에 $u_2$를 투영하면 $e_1$방향으로의 $u_2$성분이 나오는데, 그 값을 $u_2$에서 빼주면 $e_1$과 직각인 벡터가 나온다.
3. 해당 벡터에 정규화를 해주게 되면 $e_1$과 직각을 이루면서 길이가 1인 벡터를 얻을 수 있다.

**경고:** 벡터의 수가 많거나, 평행에 가까운 벡터들이 있을 때는 **그람 슈미트 알고리즘이** 최적의 방안은 아니다. **qr decomposition**같은 더 나은 알고리즘이 존재한다.

### 푸리에 변환(Fourier Transform)
**함수는 벡터일까 ? 그렇다면 왜 벡터일까 ?**
= 함수는 벡터의 조건을 만족하기 때문.

**벡터의 조건**
1. 덧셈이 정의되어 있고
2. 스칼라 곱이 정의되어 있고
3. 길이, 각도(= 내적)가 정의 가능해야 함

**함수 f(x), g(x)에 대해**
- 덧셈
    $(f+g)(x) = f(x)+g(x)$
    
- 스칼라 곱
    $(\alpha f)(x) = \alpha f(x)$
    
- 내적
    $\langle f, g \rangle = \int f(x)g(x)\,dx$

결과적으로 함수들의 집합은 **무한차원 벡터 공간**이고, **함수를 벡터로 취급**할 수 있다.

**유한 차원**의 **벡터 공간**에는 항상 **정규 직교 기저**가 존재핰다.

> 그렇다면 **무한 차원**인 **함수 공간**에도 **정규 직교 기저**가 존재할까 ?

함수 공간에도 **정규 직교 기저가 존재하며** 대표적인 기저가 $\sin(nx), \cos(nx)$이다.

선형대수에서 정규 직교 기저와, 함수 공간에서의 정규 직교 기저와 같은 구조를 띈다.


**벡터 공간에서 정규 직교 기저**
$$\mathbf{v} = \sum_i c_i \mathbf{e}_i$$

- $\mathbf{e}_i$: 정규 직교 기저
- $c_i = \langle \mathbf{v}, \mathbf{e}_i \rangle$


**함수 공간에서 정규 직교 기저**
$$f(x) = \sum_n a_n \cos(nx) + b_n \sin(nx)$$
- 기저: $\sin, \cos$
- 계수: $a_n = \langle f, \cos(nx) \rangle$


**두 구조는 완전히 같은 구조다.**

이것이 **푸리에 변환의 기본 개념**이다.

![[Pasted image 20260126183421.png]]
이를 통해서 우리는 위의 이미지를 기저 함수들의 선형 조합으로 **분해**할 수 있다.

2π 간격으로 반복되는 함수는 사인파의 기저로 투영될 수 있습니다.
$$cos⁡(nx),sin⁡(mx),m,n∈N$$
두 함수는 서로 직교하며, 자연스러운 분해를 제공한다.

![[Pasted image 20260126190639.png]]
$n,m$ 값이 **작으면**  **낮은 주파수의 진동**이 발생하고,
$n,m$ 값이 **커질수록** **높은 주파수의 진동**처럼 보인다.

**푸리에 변환**을 통해서, 이러한 **주파수들이 섞인 함수**에서 특정 주파수를 **분해**할 수 있다.
푸리에 변환은 사실 **선형 사상(linear map)** 에 불과하다.


### Frequency Decomposition of Signals
일반적으로, 신호를 서로 다른 주파수로 분산시키는 개념을 **푸리에 분해(Fourier decompositoin)** 이라고 한다.

![[Pasted image 20260126191426.png]]
이미지 처리, 렌더링, 기하학, 물리 시뮬레이션 등 모든 종류의 신호에 적용할 수 있다.


### 선형 연립방정식(System of Linear Equations)
> 좌변이 선형 함수이고, 우변이 상수인 방정식들의 모임.

**E.g**
$$ \begin{align}
x + 2y = 3, \\
4x + 5y = 6
\end{align}
$$
미지수는 DOFs(degrees of freedom), 방정식은 제약조건(constraints)라고도 한다.

**목표:** 제약조건을 동시에 만족하는 DOFs를 찾는 것.
![[Pasted image 20260127131046.png]]
[간단하게 해결할 수 있다]

**Linear System을 푼다는 것은 무엇을 의미할까 ?**

선형 시스템을 푸는 것은 여러 아핀 부분 공간이 만나는 지점(해)을 찾는 것으로 기하학적으로 해석할 수 있다.

![[Pasted image 20260127131845.png]]
모든 선형 시스템을 풀 수 있는 것은 아니다.

### Matrices

선형 대수학은 종종 행렬 관점에서 가르쳐지는 경우가 있지만,
선형 대수학의 본질은 행렬이 아니다. 행렬 없이도 기본 개념을 이해할 수 있다.
행렬은 기하학적 현상을 이해하는 데 방해가 될 수 있다.

하지만 행렬은 수치 계산에 매우 유용하며, 선형 사상을 행렬로 인코딩하여 효율적인 컴퓨터 계산을 가능하게 한다. 행렬-벡터 곱셈은 행렬의 열 벡터들의 선형 조합으로 이해할 수 있다.

![[Pasted image 20260127135536.png]]
위 이미지의 선형 사상을 행렬로 어떻게 인코딩할 수 있을까 ?

$a$벡터를 세워서 행렬로 만들면 행렬 A는 다음과 같이 만들어진다.
$$A = \begin{bmatrix} | & | \\ \mathbf{a}_1 & \mathbf{a}_2 \\ | & | \end{bmatrix} = \begin{bmatrix} a_{1,x} & a_{2,x} \\ a_{1,y} & a_{2,y} \\ a_{1,z} & a_{2,z} \end{bmatrix}$$
- **첫 번째 열:** 벡터 $\mathbf{a}_1$의 x, y, z 성분
- **두 번째 열:** 벡터 $\mathbf{a}_2$의 x, y, z 성분

이제 행렬 A와 벡터u의 계산 과정을 단계별로 살펴보자.

$$\begin{bmatrix} a_{1,x} & a_{2,x} \\ a_{1,y} & a_{2,y} \\ a_{1,z} & a_{2,z} \end{bmatrix} \begin{bmatrix} u_1 \\ u_2 \end{bmatrix}
=
\begin{bmatrix} a_{1,x}u_1 + a_{2,x}u_2 \\ a_{1,y}u_1 + a_{2,y}u_2 \\ a_{1,z}u_1 + a_{2,z}u_2 \end{bmatrix}
=
{ \begin{bmatrix} a_{1,x}u_1 \\ a_{1,y}u_1 \\ a_{1,z}u_1 \end{bmatrix} }+ { \begin{bmatrix} a_{2,x}u_2 \\ a_{2,y}u_2 \\ a_{2,z}u_2 \end{bmatrix} }
$$
행렬과 벡터를 곱하여 나온 결과를 벡터의 덧셈 규칙에 따라 두 개의 벡터로 나눌 수 있다.

$$= \space u_1 \begin{bmatrix} a_{1,x} \\ a_{1,y} \\ a_{1,z} \end{bmatrix} + u_2 \begin{bmatrix} a_{2,x} \\ a_{2,y} \\ a_{2,z} \end{bmatrix} = \space
u_1\mathbf{a}_1 + u_2\mathbf{a}_2
$$

각 벡터에서 공통된 $u_1$과 $u_2$를 벡터 밖으로 빼고 나면, 원래 정의로 되돌릴 수 있다.

행렬 연산을 할 때마다 기하학적으로 무엇을 의미하는지, 선형대수학적 관점에서 실제로 무엇을 하고 있는지 항상 고민을 해야한다.