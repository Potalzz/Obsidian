# 6.006 Spring 2020
## 1강 알고리즘이란 ?
알고리즘은 일종의 함수와 같다.
인풋을 받아와서 하나의 아웃풋을 반환하는 과정이다.

알고리즘이 효율적인지 판단하는 방법이다.
다른 알고리즘에 비해 얼마나 더 빨리 연산하는가.

효율성 시간복잡도

![[첨부 파일 소스/Image File 1/Pasted image 20240801190842.png]]
- log n은 그래프 상에서 상수보다 선형에 더 가깝다.
- 그래프에서 n을 기점으로, 왼쪽에 있는 그래프를 가지고 있는 시간 복잡도는 구리고, 오른쪽 구역의 시간복잡도를 가지고 있는 알고리즘은 좋다.

**정리**
알고리즘이란 ? 문제를 **정확하고** **효율적**이게 푸는 것.

## 2강 데이터 구조와 동적 배열열
### InterFace(API) vs Data structure
Interface는 주로 데이터 구조가 제공하는 연산이나 기능들을 의미한다.

**Interface**
>**정의**
>- 데이터 구조가 제공하는 메서드나 함수의 집합.
>- 어떤 데이터 구조가 사용자에게 어떤 기능을 제공할 수 있는지를 정의한다.

>**목적**
>- 사용자가 데이터 구조의 내부 구현 세부 사항을 알지 못하더라도 데이터 구조를 사용할 수 있게 한다.
>- 즉, 인터페이스는 데이터 구조와 사용자 간의 상호작용을 규정하는 계약이다.

>**예시**
>- 리스트의 경우, `append`, `remove`, `size` 등의 메서드가 인터페이스를 이룬다.
>- 스택의 경우 `push`, `pop`, `peek` 등이 인터페이스의 일부가 된다.

**Data Structure**:
>**정의**
>- 데이터 구조는 데이터를 저장하고 조직화하는 방식이나 방법을 말한다.
>- 특정 연산을 효율적으로 수행하기 위해 데이터를 구조화하는 방법이다.

>**목적**
>- 데이터를 효율적으로 저장, 검색, 수정하기 위해 고안된 다양한 방식들이다.
>- 데이터 구조는 내부적으로 데이터를 어떻게 관리하고, 저장하며, 접근할지를 결정한다.

>**예시**
>- 배열, 연결 리스트, 해시 테이블, 트리, 그래프 등이 데이터 구조의 예.

서로 다른 데이터 구조는 각자의 장점을 가진다.

### RAM (Random Acces Memory)
#### RAM의 정의
- 프로세서가 실행 중인 프로그램과 데이터를 임시로 저장하고 접근하는 데 사용.
- w-bit의 길이로 이루어진 array형태의 공간에 랜덤한 위치에 데이터를 저장한다.

>RAM의 각 칸을 `word`라고 부르며, 각 `word`는 w 비트만큼의 데이터를 저장할 수 있다.

>만약 16-bit words RAM이라면 각 `word`는 16비트(2바이트)의 데이터를 저장할 수 있다.

**Memory ?**
- w-bit의 크기로 이루어진 array형태의 공간

**Array ?**
- 연속적인 메모리의 조각

![[첨부 파일 소스/Image File 1/Pasted image 20240801201621.png]]
array는 결국 memory에 저장되는 것이므로, `array[i]`는 `memory[address(array) + i]` 를 의미한다.
따라서 배열의 요소를 가져오거나, 길이를 측정하는 것은 O(1)의 선형적인 시간을 가진다.
이러한 가정이 성립되려면, 처리해야 하는 데이터가 n개라고 가정하면
**w가 최소한 log2(n)이상이어야 한다.**

### 정적 배열(Static Array) vs 연결 리스트(Linked lists)
**정적 배열 (Static Array)**
- 정적 배열에서 shift를 진행하면 기존 요소들의 인덱스를 모두 변경해야 하므로 `O(n)`으로 매우 비효율적이다.
- 정적 배열은 또한 사이즈가 고정되어 있기 때문에 크기를 늘릴 수 없다.

**연결 리스트(Linked lists)**
![[첨부 파일 소스/Image File 1/Pasted image 20240805170147.png]]
-  연결 리스트는 static array처럼 일정 구간을 할당해 데이터를 저장하는 것이 아니라, 각각의 노드들을 랜덤한 위치에 저장하고 노드들끼리의 연결을 통해서 데이터를 **동적으로** 구성한다. 
- 연결 리스트에서 맨 앞에 데이터를 삽입하는 방법.
	- head pointer가 붙어있는 가장 앞의 노드에서 head 포인터와의 연결을 끊어내고, 맨 앞에 삽입할 데이터를 새로운 노드에 할당하고 head 포인터를 붙여준다. 해당 노드와 이전의 맨 앞 노드와 연결해준다.
- 결과적으로 insert-및 delete-first는 `O(1)`으로 처리할 수 있다. 하지만 일련의 포인터를 따라 데이터를 찾는 과정때문에 i번째의 데이터를 가져오기 위해서는 `O(i)`만큼의 시간이 발생한다.
- 마지막 데이터에 접근하기 위해서는 , tail이라 불리는 마지막 노드를 참조하는 포인터를 통해서 효율적으로 가져오거나 삭제할 수 있다.

### Dynamic arrays (python lists)
- 대부분의 언어에는 동적 배열인 효율적인 list가 내장되어 있기 때문에 수동으로 구성할 필요 없다.

#### Dynamic arrays의 구조
- 동적 배열은 기본적으로 정적 배열의 데이터 저장 구조를 따른다. 하지만 매번 size를 확장해야하는 정적 배열과 달리 2의 배수마다 size를 2배 확장한다. 
**사이즈를 확장하는데 드는 비용**![[첨부 파일 소스/Image File 1/Pasted image 20240805174205.png]]


**데이터 구조 비교**![[첨부 파일 소스/Image File 1/Pasted image 20240805174650.png]]

## 3강 Sets and Sorting
세트 데이터 구조에 대해 얘기함

## 4강  Hashing
  log n 보다 더 빠르게 수행할 수 있을까 ?

삽입 삭제 개선하여 구조적으로 동적으로 만들고 싶다.
이번 강의에서는 찾기,삽입,삭제 세 가지에 대해서 진행한다.

이전에 이야기했던 것보다 약한 모델을 비교 모델이라고 지칭한다.
비교 모델은 저장하고 있는 항목, 물건을 일종의 블랙박스라고 말할 수 있다.
키와 항목 또는 두 개의 항목이 주어지면 해당 키를 비교할 수 있다.
키가 숫자라면 비교과정에서  키가 숫자라는 사실만 알 수 있을 뿐이지 어떤 숫자인지는 알수 없다.

# 6.006 Fall 2011
## 1. Introduction
### 1.1 강의 소개 및 극댓값 찾기
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec01 1.pdf]]
####

간단한 알고리즘들을 점근적 복잡도 관점에서 분석해볼 것이다. 

수업을 한 문장으로 요약하면,
> 큰 입력을 받는 문제를 처리하기 위한 효율적인 방법에 관한 수업.

**Peek finding**
1차원 배열에서 정점 찾기의 의사 코드는 아래와 같다.
배열의 중간 지점에서 시작한다.
중간 지점의 양쪽을 비교해서 자신보다 큰 값이 있으면 해당 방향의 반대에 있는 부분은 모두 배제하고 남은 부분의 중앙 부분을 다시 선택하여 비교한다.
위의 비교를 `array[midIdx]`가 양 쪽 모두 자신보다 큰 값을 갖지 않을 때 까지 반복한다.

#### Recitation
해당 강의와 recitation은 점근적 복잡성(Asymptotic Complexity)에 대해서 다룬다.

![[첨부 파일 소스/Image File 1/Pasted image 20240925173501 1.png]]
점근적 복잡성을 다루기 위해 함수 $g(x)$를 살펴보자.

![[첨부 파일 소스/Image File 1/Pasted image 20240925173538 1.png]]
$g(x)$를 그래프에 그려보면 위의 꼬불꼬불한 그래프이다.

$g(x)$의 최고차 항은 $x^2$이므로 $\theta(x^2)$으로 표현을 할 수 있다.
>여기에서 $\theta()$는 무엇을 의미하는 걸까 ?

점근적 복잡성을 이야기할 때 세 가지 기호에 대해 다룬다.
- $\theta$ : $O$표기와 $\theta$표기가 같은 경우 사용
- $O$ : 상한선(upper bound)
- $\omega$ : 하한선(lower bound)

여기서 함수 $g(x)$는 적어도 $\theta(x^2)$보다 크다는 것을 의미한다.

시간 복잡도는 주로 $O$표기를 사용하며,
$\theta$표기로 나타낼 수 있는 표기를 $O$표기로 나타낸다고 생각하면 된다.

강의에서 말하는 $\theta$는 하한 경계를 의미한다.

### 1.2 계산 모델
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec02.pdf]]
####
---


## 2. 정렬&트리
### 2.1 삽입 정렬과 합병 정렬
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec03.pptx 1.pdf]]
####
---
**목차**
- 정렬
	- 삽입 정렬
	- 합병 정렬
- 재귀 풀이

>정렬은 왜 해야 할까 ?

일상생활에서 살펴 보자면, 만약 카카오톡 친구 목록이 이름 순이 아니라 무작위로 배치되어 있다면, 매번 검색해서 찾아야 하는 번거로움이 있을 것이다.
만약 여기서 큰 불편함을 못느끼겠다면 ?
채팅방이 메시지가 온 순서가 아니라 무작위의 순서로 이루어 진다고 생각해보자.`(메시지를 보낼 때마다 핸드폰을 집어 던질 것이다...)`

그렇다면 알고리즘적 측면에서 정렬되면 쉬워지는 문제들은 어떤 예시가 있을까 ?
- 중간값 또는 가장 가까운 쌍 찾기
- 이진 탐색, 통계적 이상치 확인



이 중에서 **중간값 찾기**를 살펴보자
$$A = [7, 2, 5, 5, 9, 6] \space -> \space B = [2, 5, 5, 7, 9, 6]$$
무작위의 숫자로 이루어진 배열 $A$에서 중간값을 얻고 싶은 경우 정렬된 상태라면 모든 요소를 순환해야 할 것이다.
하지만 정렬된 배열 $B$의 경우 단순히 index의 위치를 통해 상수 시간으로 중간값을 찾을 수 있다.
단순히 중간값을 찾는 것이 아니라 특정 원소의 값을 찾는 것이라고 하여도, 정렬된 배열이라면 **이진 탐색**을 통해 $O(log{n})$의 시간을 통해 해결할 수 있다.(이진 탐색은 좀 더 뒤에서 다시 살펴볼 것이다.)

>정렬은 마법처럼 이루어지는 것이 아니다...

당연하게도 정렬에는 시간이 소요 되며 정렬 알고리즘은 다양한 종류가 있다.

모든 문제들을 단순히 정렬으로만 해결할 수 있는 것은 아니지만,
데이터 압축 등 다양한 복잡한 프로그램에서 **정렬**은 다른 작업의 매우 중요한 **서브 루틴**으로 작용하므로 입력값과 데이터에 맞게 가장 효율적인 정렬 알고리즘을 사용하는 것이 중요하다.

많은 정렬 알고리즘 중 비교적 간단한 **삽입 정렬**에 대해서 살펴보자.

**삽입 정렬**
리스트를 왼쪽에서 오른쪽으로 순차적으로 탐색하며, 비교를 통해 적절한 위치에 삽입하여 작동한다.

아래의 예제를 통해 과정을 살펴보자.
![[첨부 파일 소스/Image File 1/Pasted image 20240929002839.png]]
배열 $[5,2,4,6,1,3]$이 있다.
첫 번째 요소는 정렬된 것으로 간주하고 두 번째 요소부터 확인한다.
두 번째 요소가 첫 번째 요소보다 작으므로 2를 5앞에 삽입한다.

![[첨부 파일 소스/Image File 1/Pasted image 20240929003056.png]]
두 번째 요소를 확인했으니 다음 세 번째 요소인 4를 확인한다.
두 번째 요소인 5보다 크기가 더 작고 맨 앞의 2보다는 크므로 2와 5사이에 요소를 삽입한다.

![[첨부 파일 소스/Image File 1/Pasted image 20240929003229.png]]
네 번째 요소인 6은 앞의 5보다 크고, 확인하는 요소 앞에있는 배열은 정렬되었다고 간주하므로 더이상 비교해보지 않아도 된다.

![[첨부 파일 소스/Image File 1/Pasted image 20240929003312.png]]
다섯 번째 요소인 1은 앞의 요소인 6보다 작다. 자신 보다 작은 값이 나올 때 까지 앞의 요소들을 계속해서 확인한다.
6 -> 5 -> 4 -> 2 순으로 확인이 진행되고 더이상 확인할 요소가 없으므로, 맨 앞에 1을 삽입한다.

![[첨부 파일 소스/Image File 1/Pasted image 20240929003633.png]]
마지막 요소인 3에 대해서도 위의 과정을 적용시키면 정렬된 배열 $[1,2,3,4,5,6]$을 얻을 수 있다.

**삽입 정렬**은 구현이 간단하지만 총 $\theta(n^2)$시간으로 다소 비효율 적이다.
n개의 요소를 비교하는데 각각 $\theta(n)$이 걸리므로 총 비교에 $\theta(n^2)$의 시간이 걸리고,
같은 이유로 총 스왑에 $\theta(n^2)$의 시간이 걸린다.

>비교하는데 시간을 줄여보면 어떨까 ?

 **이진 삽입 정렬**을 통해 비교 시간을 단축할 수 있다.
**이진 삽입 정렬**은 삽입 정렬과 동일한 과정으로 진행되지만, 요소를 삽입할 위치를 확인할 때 앞에 있는 모든 요소들을 확인하는 것이 아니라 **이진 탐색**을 통해 위치를 찾아내므로 각각의 비교를 $\theta(logn)$시간으로 가능하다.
하지만 총 스왑에는 여전히 $\theta(n^2)$시간이 걸리므로
**총 $\theta(n^2)$의 복잡도를 가지는 것은 동일하다.**

하지만, **합병 정렬**을 통해 정렬을 $\theta(nlogn)$의 시간으로 해낼 수 있다.

**합병 정렬**
분할 정복을 통해 배열을 정렬하며, 핵심 서브 루틴은 **합병**이다.
>**합병 정렬의 과정**
>1) 배열의 요소가 1개일 때, 정렬할 필요가 없으므로 종료한다.
>2) 그 외의 경우 배열을 반으로 나누고 재귀적으로 정렬한다.
>3) 정렬된 두 보조 배열을 **합병**한다.

아래의 예제를 통해 **합병 정렬**의 과정을 더욱 자세하게 살펴보자.
![[첨부 파일 소스/Image File 1/Pasted image 20240929010322.png]]
원래의 배열을 반으로 나눈 뒤, 두 개의 정렬된 배열로 만든다.
그 뒤 양쪽 배열의 가장 작은 값을 서로 손가락으로 짚어간다 생각하고 비교한다.

![[첨부 파일 소스/Image File 1/Pasted image 20240929010417.png]]
오른쪽 배열의 값인 1이 2보다 더 작으므로, 1을 가져와서 저장한다.

![[첨부 파일 소스/Image File 1/Pasted image 20240929010522.png]]
1을 가져와 저장했으므로 원래 있던 오른쪽 배열의 1은 사라지게 되고, 다음 손가락으로는 9를 가리킨다.

![[첨부 파일 소스/Image File 1/Pasted image 20240929010558.png]]
두 개의 값 중 왼쪽 배열의 2가 더 작으므로 2를 가져와 저장한다.

![[첨부 파일 소스/Image File 1/Pasted image 20240929010643.png]]
위의 과정을 마지막까지 진행하면 총 n개의 요소를 한 번씩만 확인하여 정렬할 수 있으므로,
**합병**에는 총 $\theta(n)$의 시간이 걸린다.

![[첨부 파일 소스/Image File 1/Pasted image 20240929011355.png]]
결과적으로 **합병 정렬**이 어떻게 $\theta(nlogn)$의 복잡도를 가지는지 재귀 트리를 통해 살펴보자.

**합병 정렬**의 재귀식, n개의 항목에 대하여 $T(n)$을 아래와 같이 작성할 수 있다.
![[첨부 파일 소스/Image File 1/Pasted image 20240929011547.png]]
위의 식에서 가장 높은 차수 항은 **합병**에 걸리는 시간인 $c \cdot n$이므로 나머지 상수항은 무시해도 된다.

$c \cdot n$에 대한 재귀 트리를 살펴보자
![[첨부 파일 소스/Image File 1/Pasted image 20240929012517.png]]
재귀를 진행하여 2등분 하여 아래로 계속해서 내려오다 보면,
n은 결국 1의 값을 가지게 될 것이고 맨 아래 leaf 노드에는 상수인 c만 남는다.

재귀 트리의 높이는 n이 1이 되기까지 총 $1 + logn$의 높이를 가질 것이고 단말 노드의 개수는 n개가 될 것이다.

트리의 각각 단계에 작업을 진행해보자.
![[첨부 파일 소스/Image File 1/Pasted image 20240929012624.png]]
각 단계는 모두 $cn$만큼 작업을 하며, 트리의 높이인  $1 + logn$번 진행한다.
그렇다면 점화식을 다음과 같이 정의할 수 있다.
$$T(n) = (1 + logn) \cdot  cn = \theta(nlogn)$$

결과적으로 **합병 정렬**의 복잡도가 $\theta(nlogn)$인 것을 알 수 있다.

#### 🔍 수업에서 무엇을 얻었나
정렬을 왜 사용하고 정렬은 어떠한 과정에서 이루어 지는지 알 수 있다.
그냥 정렬을 사용하는 것과, 원리를 알고 이해한 뒤에 사용하는 것은 천차만별이라고 생각한다.


### 2.2 힙 & 힙 정렬
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec04.pptx.pdf]]
####
---


### 2.3 일정 예약과 이진 탐색 트리
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec05.pdf]]
####
---

#### Relation
![[첨부 파일 소스/PDF File 1/rec05.pdf]]

![[첨부 파일 소스/Image File 1/Pasted image 20240927173828.png]]
**지수의 역수는 로그이다.**

Data structure에서 query는 작업을 수행하기 위한 명령을 의미한다.

**BST**는 트리가 불균형으로 이루어져 있다면, 탐색에 $O(h)$가 걸린다. 이는 root노드의 child 노드들이 연속해서 커지는 값으로 이루어진 경우(1, 2, 3, 4, 5 ...) 최대 높이는 $n$이다.
이러한 문제를 해결하기 위해서 불균형 트리를 균형 이진 트리인 **AVL**트리로 변환해줘야 한다.


### 2.4 균형 이진 탐색 트리 (AVL)
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec06.pdf]]
####
---


### 2.5 선형 시간 정렬(Linear Time Sort)
#### 강의 노트
![[첨부 파일 소스/PDF File 1/bf7d79105762bf79bbc0925438e1468a_MIT6_006F11_lec07.pdf]]
####
---


**강의 목표**
lower bounds에서
- searching : Ω (log n) (이진탐색의 최적)
- sorting : Ω (n log n) (합병 정렬의 최적)
두 가지를 증명해보고

`비고 모델(Comparison Model)`이 아닌, `일반적인 RAM모델`에서 특정 조건에서 선형시간 정렬을 증명해볼 것이다. = `O(n)정렬`

**비교 모델**
비교 모델의 아이디어는 비교를 할 때 어떤 연산을 사용할 지 제한하는 것이다.

모든 입력 항목이 무엇인지 정확히 모름 `black boxes(값이 정확히 몇인지 모름)`
black boxes는 추상 장료형(ADTS) 
주어지는 모든 항목은 일종의 자료 구조형이고 자로 구조를 정렬하고 싶다.
비교 모델에서 유일하게 가능한 연산은 비교 연산자`(<, >, >= ...)`뿐이다.
힙, 힙 정렬, 이진 탐색 모두 비교 만으로 항목을 다루는 비교 모델에 속한 알고리즘이다.
시간 비용은 당연히 비교한 횟수로 계산한다.
- time cost = `#comparisons`

**Searching lower bound** 
- n개의 전처리된 아이템들.
- 비교를 통해 자리를 올바른 자리를 찾아감.
- `lower bound(해당 시간보다 적게 나올 수 없음. 즉, 하한)`가 Ω log(n)인 이유는, 값을 올바르게 넣기 위해서는 트리의 root에서 leaf까지 비교를 진행해야 하는데 해당 트리의 높이가 log(n)에 해당하기 때문이다.

**Linear - Time sorting**
- **정수 정렬** 매우 크지 않은 수 k이내에 속하는 `정수`인 수에만 적용 가능하다.
- 비교연산이 아니므로 비교 연산자 외의 모든 연산자를 사용할 수 있다.

## 3. 해싱
### 3.1 해싱 1 [8/20]
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec08.pdf]]
####
---

딕셔너리는 매우 많은 곳에서 쓰인다.

**딕셔너리는 데이터를 어떻게 저장할까 ?**
1부터 n까지 엄청나게 큰 배열을 만들고 주어진 key에 값을 저장한다.

**위 방식처럼 동작할 시 발생할 수 있는 문제점은 ?**
1. 키가 정수가 아닐 수 있다.
2. 어마어마한 메모리 낭비 발생.

**해결방안**
#### 프리 해시
- 프리 해시는 키가 어떤 값이든 간에 정수로 대응시켜 저장한다.
- 프리해시의 키는 **바뀌면 안된다.** 값을 저장하고 key가 바뀌면 항목을 찾을 수 없게 되기 때문.

##### 해싱(hashing)
 - 해싱의 핵심 개념은 **해시 함수**를 통해 모든 가능한 키를 작은 정수 집합으로 줄이는 것이다.
>**해시 함수** 
>	**ㄴ>데이터를 고정된 크기의 숫자나 문자열로 변환하는 함수**
>1. 전체 집합을 `0, 1, ..., m-1`에 대응시켜 키의 공간을 m개의 슬롯으로 줄여준다.
>2. 해시 함수의 종류는 한 가지만 있는 것이 아니며, 테이블의 크기에 따라 여러개가 있을 수도 있다. 상황에 맞게 알맞은 해시 함수를 사용해야 한다.

 >**해시 함수의 예시 (division method)**
> 모든 키가 자연수일 경우 문자열을 모두 아스키 코드에 따라 정수로 변환한다. 변환한 정수를 k라고 할 경우 `h(k) = k % m`으로 항상 0 ~ m-1 사이의 정수가 된다.

 **해싱을 통해 공간을 축소하는 과정**
- 매우 큰 keyspace 공간을 만들어준다. 그 안에서 필요한 k의 개수만큼의 공간을 따로 만들어준다. 그 뒤 h함수에 각각의 키값을 대입해 table에 삽입한다. (4p 그림.2 참고)
	ex) `k1 => h(k1) => 1
- 위의 방법으로 진행했을 때 h함수에 값을 넣은 결과가 같은 값이 나왔을 때의 경우인 **충돌**이 발생할 수 있다. `ex) h(k1) = h(k2) = 2`

**충돌 해결 방안**
**1. 체이닝**
- 여러 항목이 테이블의 같은 위치에 있을 때 `Linked list`로 저장하는 것.
- 최악의 경우 모든 키가 같은 슬롯에 배정될 수 있다. 이러한 경우 n개로 이루어진 리스트가 생성되므로 `O(n)`
- 기본적으로 해시 함수는 무작위화를 진행하여 항목을 잘 나누어주기 때문에 대부분의 경우에는 `상수시간 O(1)`으로 작동한다.

앞의 과정으로 **해싱**을 통해 **공간**을 **축소**하는 과정을 증명하였다.
**해싱**을 통해 어떻게 **시간**을 **축소**시킬 수 있을까 ? => **SUHA** 가정을 통해 증명.

**SUHA(Simple Uniform Hashing Assumption)**
- 각각의 키가 모든 슬롯들에 **균등**한 확률로 **독립적**으로 해싱된다는 가정.
- **SUHA** 는 확률적 가정 두 개로 이루어져 있다.
- *1. 균일성(Uniformity)*
	-  공간에서 키를 골라  해시 테이블에 저장하고 싶을 때 해시 함수는 균일하고 무작위적으로 선택하여 대응시킨다. 각 슬롯은 동등하게 해시된다.
- *2. 독립성(independently)*
	- 앞의 키가 어떤 슬롯으로 갔든지 각각의 키들에 해시 함수는 독립적으로 적용된다.
	- `n개의 키`와 `m개의 슬롯`이 있을 경우 예상되는 `체인의 길이`는? => `n / m`
	- 그 이유는 `키`가 어떤 슬롯에 들어갈 확률은 `1 / m`이며, 모두 독립적으로 작동하면 `1 / m * n = n / m`이 된다.
	- 하지만 현실에 독립성은 존재하지 않으므로 다음의 가정을 따른다.
	
	- `n/m` = `α(체인의 길이)` = `테이블의 적재율(테이블의 크기에 비례하는 n의 개수)`의 값이 1일 경우 `n = m`이므로 `체인의 길이`는 1이다. `키`가 `슬롯`보다 10배많을 경우는 10이므로 `체인의 길이는 10`. 즉 `체인의길이 α`는 상수이다.
	- **α가 상수임을 대입하여** 모든 연산의 실행 시간은 **O(1 + chain의 길이)** 이므로, **O(1 + α)** 이다. 즉, 기대 시간은 **상수 시간**.

- **결론 :** `n = O(m)`이면 평균 검색시간은 `O(1)`이다. **SUHA**가 성립되는 경우 모든 연산이 `상수시간 O(1)`으로 이루어진다는 **가설**.

> **그래서 해시 함수 h를 어떻게 만드는데 ?**

***해시 함수의 3가지 예***
>**분할 메소드(division method)**
>`h(k) = k mod m (= k % m)`
>키 k가 있을 때 k를 m으로 나눈 나머지를 구하는 방식.
>m이 2의 제곱이나 10의 제곱에 가까이 있지 않은 소수일 경우 실용적이다.
>**단점 :** m과 k가 공약수를 가질 때 좋지 않다.

>**곱하기 메소드(multiplication method)**
>`h(k) = [(a*k) mod 2^w] >>> (w-r)`
>	**ㄴ>** `a는 임의의 정수, k는 w 비트, 그리고 m = 2^r이다.`
> 키 k가 있을 때 k에 정수 a를 곱한 뒤 mod 2의 w제곱을 한 뒤 오른쪽으로 w-r만큼 이동. ![[첨부 파일 소스/Image File 1/Pasted image 20240821130614.png]]
> 먼저 w 비트 길이의 k와 w 비트 정수 중 랜덤한 수인 a를 곱한다.
> 이진에서 곱셈이란 ? => a의 1마다 k를 복사하는 것을 의미한다.
> k와 a를 곱해서 나온 값은 일반적으로 워드 길이의 2배이다.
> 해당 값에서 mpd 2^w를 통해 골라낸 값을 r이라고 하고, r을 오른쪽으로 w - r 만큼 이동. 즉, 맨 오른쪽으로 보낸다.
> r은 0과 m-1 사이에 위치한다.
> 
> 이 방법은 a가 홀수 & 2^w-1 < a < 2^w & a가 2^w-1 또는 2^w에 너무 가깝지 않아야 실용적이다.

> **유니버셜 해싱(Universal hashing)**
> `h(k) = [(ak + b) mod p] mod m`
> p는 전체집합 크기보다 큰 소수를 의미하며, a와 b는 0 ... p-1사이에 있는 임의의 수이다.
> **최악의 경우**에 두 개의 키가 충돌할 확률은 1/m이다.
> 왜 최악의 경우인가 ? -> a와 b라는 무작위적인 부분이 존재하기 때문. 이 확률은 a/b와 같다.
> 세부 정리는 6.046에서 확인할 수 있다.


### 3.2 해싱 2 [8/25]
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec09 2.pdf]]
####
---


**지난 수업 복습**
n개의 키는 m크기의 테이블로 들어간다. 키가 테이블의 같은 슬롯으로 들어가는 경우 체이닝 되어 Linked list로 연결되며, 순서는 중요하지 않다. 키는 균등하게 독립적으로 체이닝되므로 슬롯에 들어갈 확률은 `1/m(테이블의 크기)` 이다. 키가 n개만큼 있으므로  체인 길이의 기대값은 `n/m`이다. hash의 모든 연산은 Θ(1+ `α(체인의 길이)`) 이므로 체인의 길이가 상수이기 때문에 시간 복잡도의 기대값은 상수시간 Θ(1)이다.

이번 수업은 해시 함수에 대해서는 크게 다루지 않으며, 해시 함수 두 가지만 기억하자.
1. 나머지 함수 (division method)
	모든 정수를 0부터 m-1까지 원하는 크기의 집합으로 만들 수 있는 쉬운 함수.


2. 곱하기 함수 (multiplication method)
	무작위의 정수를 곱한 뒤 무작위로 키가 섞인 중간 부분을 쓰는 해싱 방법.

---

#### m의 적정 크기는 얼마인가 ?
m이 너무 작으면 체인의 길이는 상수시간을 넘어서게 되고,
m이 너무 클 경우 공간 낭비가 심하다.
**want m = Θ(n) => α => Θ(1)**

해시 테이블의 크기를 n에 맞게 늘린다.
먼저 m을 아무 상수로 정한다.
n에 맞게 m의 크기를 늘이거나 줄인다.

**롤링 해시 ADT**는 **카프 - 라빈 알고리즘**을 위한 도구로 **카프-라빈 알고리즘**의 성능을 최적화하는데 사용된다.


### 3.3 해싱3 : 개방 주소법(open addressing) [8/25]
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec10.pdf]]
####
---

해싱을 가장 처음 배울 때 충돌을 해결하는 방법에는 두 가지 방법이 있다고 했다.
**체이닝**과 **개방 주소법**이다. **체이닝**은 포인터를 활용해 구현하는 방법이고, **개방 주소법**은 배열을 통해 구현하는 방법이다.

#### 개방 주소법
- 해시 테이블을 쓰는 것과 해싱에 대해서 한 가지만 기억하라면 그것은 바로 **개방주소법**이다.
- 해시 테이블을 구현하기 가장 쉬운 방법이다. 배열을 써서 해시 테이블을 구현할 수 있음.
- 하나의 배열 구조를 통해 해시 테이블을 구현. 
- 균일 가정이 필요하다. (이전 체이닝에서 나온 균일 가정과는 다른 가정)
 
하나의 배열이 있고, 한 슬롯에는 최대 하나의 요소만 담길 수 있다. 체이닝을 통해 linked list로 슬롯의 크기를 늘리는게 불가능하므로 `m(테이블의 크기) >= n(아이템의 개수)`
위의 방식으로 작동하기 위해서 **조사(probing)** 라는 개념을 사용한다.

**조사(probing)**
- 해시 테이블에 데이터 삽입을 할 수 있는지 살피고 불가능한 경우, 약간 다른 해시값을 구한다.

여기서 사용되는 해시 함수는 체이닝의 해시 함수와는 다르게 적용된다. 해시 함수는 조사할 슬롯의 순서를 정한다.
또한 **탐색/삽입/삭제** 3가지 경우에서 조금씩 다르게 적용된다.
해시 함수 h는 인수로 `키`와 `시도 횟수`를 받아 `0 ... m-1`사이의 해시 값을 반환한다.

> **해시 함수 h = `h(key, trial count)`**

최종적으로 `0 ... m-1`까지 모든 테이블에 값이 채워져야 한다.
**ex)** 해시 테이블에 486이라는 값을 넣고 싶을 때, 한 번의 시도만에 테이블의 빈공간인 2번째 칸에 삽입됐을 경우 `h(586,1) = 2`
만약 삽입에 실패했을 경우, 시도 횟수를 1씩 늘려가면서 시도한다.
 
**삽입(insert)**
- 시도 횟수를 늘려가며 키가 빈 슬롯에 삽입될 때 까지 계속해서 시도한다.

**탐색(search)**
- 시도 횟수를 늘려가며 슬롯의 해시값과 키가 동일한지 확인한다. (세가지 경우에 따라 행동)
- **(1) 해시값이 동일한 경우 :** 값을 반환하면 된다.
- **(2) `None`이 나온 경우 :** 키가 테이블에 없는 것이다.(빈 슬롯이 있었다면 이미 삽입 때 삽입되었을 것이므로)
	**하지만 만약 중간에 테이블의 키가 삭제되어서 테이블에 키가 존재함에도 None을 반환한다면 ?**
	- 삭제가 진행된 슬롯을 확인하기 위해서, 삭제된 슬롯에는 `None` 대신  `Delete Me`를 부여한다.
- **(3) `Delete Me` 가 나온 경우 :** 해당 슬롯은 건너뛰고 계속 탐색을 진행한다.

**삭제(delete)**
- 탐색을 통해 값을 찾고, 해당 슬롯에 `Delete Me`를 부여한다.


**조사 전략(Probing Strategy)**
- 해시 함수를 개방 주소법에 사용할 수 있게 바꾸는 일.
- **(잘못된 예시)** **선형 조사(Linear probing) :** `h(k,i) = (h'(k) + i) mod m` 
	- **선형 조사의 문제점 :** 시도 횟수`i`마다 값을 1씩 더해주므로 **군집화**가 발생된다.`(3개의 슬롯에 값이 연속적으로 있는 경우, 3개중에 하나의 슬롯에 걸리기만 해도 해당 키는 무조건 군집의 4번째 값으로 배정된다)` 이는 곧 평균 상수시간의 성질을 잃어버리게 된다.
- **이중 해싱(Double hashing) :** `h(k,i) = (h1(k) + i * h2(k)) mod m`
	- 이중 해싱이 순열의 조건을 만족하기 위해서는, `h2(k)`와 `m`이 **서로소(공약수가 1뿐인 두 정수)** 관계에 있어야 한다.
	- `m = 2^r(2의 제곱수) , h2(k) = 홀수`이면 순열 조건을 만족한다. 그러기 위해 배열의 크기를 2의 제곱수로 설정한다.

개방 주소법이 체이닝만큼 잘 동작하는지 증명하려면 **균일 해싱 가정**이 필요하다.
**균일 해싱 가정**은 체이닝의 키가 슬롯에 독립적으로 배정되는 **단순 균일 해싱**과는 다르다.

**균일 해싱 가정(Uniform Hashing Assumption)**
- 각 키가 가지는 조사 순서가 m!의 무작위 순열 중에 특정 순열일 확률이 균등하다.
	- 사실 이렇게 될 수는 없고, **이중 해싱**을 사용하여 비슷하게 할 수 있다.
- `α(적재율) = n/m`일 때, **연산의 비용**은  `1 / (i-α)`보다 **작거나 같다**.
- 적재율이 커질수록 연산의 평균 비용은 높아진다. 일반적으로 적재율이 0.5이상이면 좋지 않음.
- **예시 :** α가 0.9일 때 기대 조사 횟수는 10이다.

첫 시도만에 키가 슬롯에 배정될 확률 `P`는 `P` = `(m-n) / m` = `1 - α`

#### 개방 주소법 vs. 체이닝
**개방 주소법:** 더 나은 캐시 성능 (더 나은 메모리 사용, 포인터를 쓸 필요 없음)
**체이닝:** 적재율`(개방 주소법은 적재율이 70%을 넘으면 저하된다. 적재율이 1을 넘으면 안된다)` 과 해시 함수`(개방 주소법은 군집화를 막아야 한다)` 에 제한이 적다.

#### 암호학적 해시
단방향 해시 함수를 구현한다.
- **단방향 해시 함수 :** 비밀번호 x가 있을 때, `h(x)`에서 `x`의 값을 알기 어렵게 만드는 것이다.
- 홈페이지에서는 비밀번호를 그대로 저장하지 않고 `h(x)`의 결과값만 저장해 새로 입력하는 비밀번호와 맞는지 비교한다.

---

## 4. 수 - RSA 암호
### 4.1 수 - RSA 암호 수1 [8/27]
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec11.pdf]]
####
---


RSA암호화와 관련된 다른 응용법은 수천 비트의 길이를 가지는 소수를 이용하여 동작한다.

> 이렇게 길이가 긴 수를 곱하거나 나누려면 어떻게 해야 할까 ?

**뉴턴 방법**을 통해서 무리수를 임의의 정밀도로 계산하는 방법을 살펴보고, 수천 비트 길이의 수를 나누고 곱하는 방법에 대해서 알아보자.

##### 💡 뉴턴법(Newtons's Method)의 기본 개념
**목표**
- 비선형 방정식 $f(x)$ = 0의 해를 찾는 것.
- 일련의 반복 과정을 통해 함수의 접선을 이용하여 $f(x) = 0$을 만족하는 $x$값을 찾는다.

**알고리즘**
- 뉴턴법은 함수 $f(x)$ 의 해를 찾기 위해, 함수의 값과 함수의 `도함수(미분값)`를 사용하여 반복적으로 근을 개선해 나가는 방법이다.

##### 📚 뉴턴법의 수학적 배경
- 뉴턴법은 함수 $f(x)$의 그래프에서 접선의 기울기를 이용하여 함수의 해를 찾는 방법이다. 접선의 방정식은 다음과 같다.

$$f(x) ≈ f(x_n) + f'(x_n) ⋅ (x - x_n) $$
이 접선이 $x$축과 만나는 점이 $x_{n+1}$이며, 이를 통해 새로운 추정값을 구한다.

##### ⚒️ 식의 구성
**현재 추정값 $x_i$**
- $x_i$는 현재 반복 단계에서의 근의 추정값이다. 즉, 방정식의 해가 될 것으로 예상되는 값.

**함수 값 $f(x_i)$**
- $f(x_i)$는 현재 추정값 $xi$에서 함수 $f(x)$의 값이다. 이 값은 $f(x_i)$가 0에 얼마나 가까운지를 나타낸다.

**도함수 값 $f'(x_i)$**
- $f'(x_i)$는 함수 $f(x)$의 현재 추정값 $x_i$에서의 `미분값(도함수)`이다. 도함수는 함수의 기울기를 나타내며, 방정식의 해를 찾기 위한 기울기 방향을 제공한다.

**새로운 추정값 $x_{i+1}$**
- $x_i+1​=x_i​−f(x_i​)/f'(x_i​)​$는 새로운 추정값으로, 현재 추정값 $x_i$에서 함수의 값과 기울기를 이용하여 보다 정확한 해를 추정한다.

##### 📃 설명
1. **근의 추정**
	- $x_i$는 현재 반복 단계에서의 해의 추정값이다. 초기 추정값 또는 이전 단계의 결과로부터 얻어진 값

2. **오차 수정**
	- $f(x_i) / f'(x_i)$는 함수 $f(x)$의 기울기를 고려하여 추정값 $x_i$에서의 오차를 수정. $f(x_i)$가 함수의 값이기 때문에, 이 값은 현재 추정값이 해에서 얼마나 떨어져 있는지를 나타낸다.
	
3. **반복 업데이트**
	- 이 식을 사용하여 새로운 추정값 $x_{i+1}$을 계산하고, 이 과정을 반복하면서 해에 점점 더 가까워지도록 한다.

##### 🔎  예시
식 $f(x) = x^2 - a$가 있을 때 연속 근사법을 이용하여 $f(x) = 0$의 해를 찾는 과정을 살펴보자.

![[첨부 파일 소스/Image File 1/Pasted image 20240829203824.png]]

1. **초기 추정값** : $x_0$를 선택한다.
2. **반복**
	- 계산 : $x_{i+1}$ = $x_i$ - $f(x_i) / f'(x_i)$
	- 업데이트 : $x_i$를 $x_{i+1}$로 업데이트한다.
	- 이 과정을 $f(x_i)$가 0에 매우 가까워질 때까지 반복한다.

점 $(x_i, f(x_i))$에서의 접선은 $y = f(x_i) + f'(x_i)\cdot(x-x_i)$이고, $f'(x_i)$는 도함수(= $f(x_i)$의 미분 값) 이다. $x_{i+1}$은 $x$축과의 교점이 된다.
$$x_{i+1} = x_i - f(x_i)/f'(x_i) $$
이를 통해 얻은 $x_{i+1}$의 값을 $x_i$에 대입하고, 해당 과정을 반복해 나가게 되면, $f(x_i)$가 점점 0에 가까워지는 값을 얻어나갈 수 있다. 

![[첨부 파일 소스/Image File 1/Pasted image 20240829204442.png]]

여기서 $a$는 작은 정수이다. 해당 식에서 $a = 2$인 경우에 값을 업데이트해 나가면 $\sqrt{2}$ 의 근사값을 구할 수 있다.

![[첨부 파일 소스/Image File 1/Pasted image 20240829204900.png]]

$i$가 1씩 증가할 때마다, 수는 **이차적 수렴**하며 $f(x) = 0$을 만족하는 해의 정밀도를 가지게 된다.
> **이차적 수렴 :** 자릿수의 개수가 두 배로 늘어남을 의미.
> 1 -> 1.5(1자릿수 정밀도) -> 1.41(2자릿수 정밀도) -> 1.4142(4자릿수 정밀도) -> 1.41421356(8자릿수 정밀도) ...

따라서 자리수의 개수가 두 배로 늘어나므로, **뉴턴 방법**을 통해 계산을 계속해나가기 위해서는 **고정밀 나눗셈**을 계산해야 한다.

>**왜 뉴턴 방법에서 고정밀 나눗셈이 필요한가 ?**
>$x_{i+1} = (x_i + a/x_i) / 2$에서 $a / x_i$를 계산해야 하는데, $x_i$의 자릿수는 지수함수적으로 증가하므로, 매우 큰 자릿수를 가지는 수가 된다.

대부분의 나눗셈 알고리즘은 서브 루틴으로 곱셈이 사용된다.
> $a / x_i$는  $x_i$의 역수에 $a$를 곱하는 것과 동일하기 때문.

**고정밀 나눗셈**을 하기 위해 먼저 **고정밀 곱셈**에 대해서 먼저 알아보자.

##### 고정밀 곱셈
두 개의 $n$자릿수 수인 $x$와 $y$를 곱하려고 한다. (해당 수는 2진수 or 10진수)
- **기본 범위** : $0 ≤ x,y ≤ r^n$

**목표**
- **분할 정복(divide & conquer)** 을 통해서 $r$의 지수인 $n$은 큰 수이므로 $n$을 쪼개서 두 수의 곱으로 만들 것이다.
- 계속해서 쪼개나가면 `64비트`의 수를 얻게 될텐데, 그렇게 되면 컴퓨터에서 한 번의  지시만으로 `64비트` 숫자들의 곱셈을 수행할 수 있다.

> $$x = x_1 \cdot r^{n/2} + x_0 \space,\space\space y = y_1 \cdot r^{n/2} + y_0$$
>$x$를 구하기 위해서 $x$에 대한 식을 성립한다.
>$x_1$은 상위의 절반이고, $x_0$은 하위의 절반이다.
>$y$에 대해서도 동일하게 식을 성립한다.

>$$0 ≤ x_0, x_1 ≤ r^{n/2} \space,\space\space 0 ≤ y_0, x_1 ≤ r^{n/2}$$
>$x_0$과 $x_1$의 범위는 위와 같다. ($y$도 동일)


>$$z_2 = x_2 \cdot y_2 \space,\space\space z_1 = x_1 \cdot y_0 + x_0 \cdot y_1 \space,\space\space z_0 = x_0 \cdot y_0$$
>식의 곱셈을 간단한 형태로 쪼갠다.
>계속 적용해 나가다 보면, 재귀적인 형태의 곱셈을 만들 수 있다.

>$$z = z_2 \cdot r^n + z_1 + z_0$$
>$z$에 대한 식은 위와 같다.

>$$z = x\cdot y = x_2 \cdot y_2 \cdot r^n  + (x_1 \cdot y_0 + x_0 \cdot y_1) + x_0 \cdot y_0$$
>$x$와 $y$에 $z$대신 $x$와 $y$를 대입하여 식을 수정.

>$$T(n) = 4 \cdot T(n/2) + \theta(n)$$> 해당 식에서의 점화식 

이는 각 $n/2$ 자리의 숫자를 4번 곱해야 한다는 사실을 알 수 있고,
결과적으로 이 계산은 $\theta(n^2)$ 시간이 걸린다고 말할 수 있다.

하지만, 결과적으로$\theta(n^2)$ 보다 더욱 나은 성능의 알고리즘을 얻고 싶다.
복잡도를 낮출 수 있는 간단한 방법으로 **카라추바 알고리즘**을 통해 해결할 수 있다.

##### 카라추바 알고리즘(Karatsuba Algorithm)

>$$z_2 = x_2 \cdot y_2 \space,\space\space z_0 = x_0 \cdot y_0$$
> $z_2$와 $z_0$까지는 위의 계산과 동일하지만, $z_1$의 경우에 조금 다르다.

>$$z_1 = (x_0+x_1) \cdot (y_0+y_1) - z_0 - z_2 $$
>$z_0$과 $z_2$를 먼저 계산하고, 이를 통해 $z_1$을 계산한다.

$z_1$의 식을 위와 같이 변경하여 계산하게 되면, 곱셈의 횟수가 총 **4번**에서 **3번**으로 줄어든다. 

>$z = z_2 \cdot r^n + z_1 + z_0$ 을 **총 3번**의 곱셈을 통해 해결.
>1. $(x_0+x_1) \cdot (y_0+y_1)$
>2. $z_0$ 에  $x_0 \cdot y_0$ 대입하여 계산
>3. $z_2$ 에  $x_2 \cdot y_2$ 대입하여 $x_2 \cdot y_2 \cdot r^n$ 계산

>$$T(n) = 3 \cdot T(n/2) + \theta(n)$$> 해당 식에서의 점화식은 위와 같다. (덧셈은 n자리 숫자도 선형시간에 해결 가능.)

결과적으로 $\theta (n^{log3})$ = $\theta (n^{1.58..})$의 시간복잡도를 얻어낼 수 있다.

##### ⭐ 결론
- 뉴턴 방법의 핵심은 현재 추정값 $x_i$와 함수 $f(x)$의 기울기 $f'(x_i)$를 사용하여 보다 정확한 다음 추정값 $x_{i+1}$을 계산해 나가며 고정밀도의 소수점을 가지는 수를 얻어낸다. 

---

### 4.2 수 - RSA암호
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec12.pdf]]
####
---


아직 뉴턴법을 사용할 때 필요한 나눗셈은 배우지 않았다.
또한 전체 알고리즘의 복잡도에 대해서도 아직 모른다. 단지 곱셈 연산의 복잡도에 대해서만 배웠을 뿐.

#### 이번 강의의 목표
**제곱근을 계산하는 것의 복잡도 알아보기**
제곱근을 계산할 때는 몇 번의 곱셈이 필요할까 ?
곱셈이 실제로 사용되는 부분은 제곱근을 계산할 때 나눗셈이 필요한 부분이다. 해당 계산엔 두 가지 단계가 있으며, 해당 단계를 구체적으로 살펴봄으로써 전체 알고리즘의 복잡도를 알아내는 것이 이번 강의의 목표이다. 

$x$의 n번째반복에서 $x_n$은 $\sqrt a (1+En)$이다.
$En$은 양수거나 음수이며 $n$번째 반복에서 $x_n$이 가지는 오차를 의미한다. ($x_n = \sqrt a + \sqrt a \cdot En$)

$x_n$을 $x_i$에 대한 식에 대입하여 $x_{n+1}$의 식을 구해보자.
![[첨부 파일 소스/Image File 1/Pasted image 20240829204442.png]]

![[첨부 파일 소스/Image File 1/Pasted image 20240901142701.png]]

![[첨부 파일 소스/Image File 1/Pasted image 20240901142723.png]]

> 식을 간단하게 하다보면 결과적으로 위의 식을 도출해낼 수 있다.

![[첨부 파일 소스/Image File 1/Pasted image 20240901143005.png]]
$E_n$은 점점 작아지는 성질을 가지고 있으므로, $E_n$의값은 0으로 수렴하게 된다.$E_n^2$과 $E(n+1)$의 관계를 통해,  $E_n$이 0.1일 경우 $E_n^2$은 0.001이므로 이차적 수렴의 성질을 가지게 된다.

자리수는 이차적 수렴의 성질을 가지고 있으므로, $d$만큼의 자리수를 구하고 싶다면, $log_2d$만큼 반복하면 된다.

이제 반복 횟수를 알았으므로 고정밀 나눗셈의 복잡도를 알아낸다면, $\sqrt 2$나 다른 제곱근 값을 뉴턴법으로 계산할 때 복잡도를 알아낼 수 있다.

#### Toom-Cook 알고리즘
**카라추바 알고리즘**을 일반화한 방식으로, 기존 $x$와 $y$를 상위, 하위 총 2부분으로 나눈 것에 비해 상위, 중간, 하위 3가지 부분으로 나누어서 계산하는 방식이다. $x_2, x_1, x_0$과 $y_2, y_1, y_0$에 대해 $d/3$자릿수 숫자들을 서로 곱하려면 총 **9번**의 곱셈이 필요하다. **카라추바 알고리즘**에서 보여줬던 산술적인 기교를 통해 **9번**의 곱셈을 **5번**으로 줄일 수 있다.
![[첨부 파일 소스/Image File 1/Pasted image 20240901144800.png]]
- 결과적으로 **Toom3**알고리즘의 점화식은 위의 형태를 가진다,
- 해당 알고리즘에서 $n$자리수를 가지는 두 수 $x$와 $y$를 곱하는데 걸리는 복잡도는 ?
- $\theta (n^{log3^5}) = \theta (n^{1.465...})$의 복잡도를 가지게 된다.

#### Schonhage-Strassen (고속 퓨리에 변환)
$$\theta (n \space log^n \space loglog^n)\space time$$
고속 퓨리에 변환, 즉 FFT를 사용하여 거의 선형시간에 가까운 복잡도를 가지는 알고리즘.

#### 고정밀 나눗셈
**$a$를 $b$로 나누는 고정밀 나눗셈.**
$b$의 역수를 구해 $a \cdot 1/b$를 해야하기 때문에, 1을 $b$로 나누는 나눗셈을 계산해야 한다.

##### 1/b를 구하기 위해 r/b를 하는 이유
강의 중 내용 [25:00]
>r의 거듭제곱 진법에서 r로 나누는 것은 매우 쉬운 일이다 예를 들어 r이 2^k 라고 가정하면, 2진법을 사용하는 경우에 시프트 연산으로 쉽게 나눗셈을 할 수 있다. r을 b로 나눈 값을 계산하기 위해서 2진법으로 된 수백만 자리의 숫자가 주어진다면 이것을 적절한 만큼 r나누기 b의 결과에 시프트시켜서 1/b의 값을 구할 수 있다.

**용어 정리**
1. **r의 거듭제곱 진법**: 여기서는 r이 2^k로 설정됩니다. 예를 들어, r=2^4=16이라면, 숫자는 16진법과 비슷한 형태로 표현됩니다.
    
2. **2진법**: 모든 숫자가 0과 1로만 표현됩니다.
    
3. **시프트 연산**: 2진법에서 시프트 연산은 숫자를 왼쪽 또는 오른쪽으로 이동시키는 것입니다. 예를 들어, 숫자 1100을 오른쪽으로 1비트 시프트하면 0110이 됩니다. 이는 곧 그 숫자를 2로 나눈 것과 같습니다.
    

**기본 개념**

1. **r로 나누는 것이 쉬운 이유**:
- r이 2^k인 경우, 2진법에서 r로 나누는 것은 매우 쉽습니다. 왜냐하면 이진수에서 r=2^k로 나누는 것은 그저 비트를 오른쪽으로 k번 시프트하면 되기 때문입니다.
- 예를 들어, r=16=2^4이라면, 16으로 나누는 것은 비트를 4번 오른쪽으로 시프트하는 것과 같습니다.
> 예를들어 2진법 숫자 1010101 이 있을 때 해당 자릿수의 숫자는 2^자리수 * 숫자에 해당하므로 이 수들을 모두 더해주면 10진법의 숫자가 된다. 
> 맨 오른쪽 부터 2^0 * 1 + 2^1 * 0 + 2^2 * 1 + ...

2. **r을 b로 나누는 것**:
- 여기서 b는 나누려는 숫자입니다. r÷b를 계산해야 하는데, r이 2^k로 매우 크기 때문에 r과 비교해서 b는 상대적으로 작은 숫자일 수 있습니다.
- r÷b는 고정된 상수로 계산할 수 있습니다.

3. **2진법 수를 r로 나눈 후, 다시 1/b를 구하는 방법**:
- 수백만 자리의 2진법 숫자가 주어진다면, 이 숫자를 r로 나눈 후, 나눈 결과를 시프트 연산으로 간단하게 처리할 수 있습니다.
- 예를 들어, r÷b의 결과가 4라면, 이를 시프트 연산으로 쉽게 적용할 수 있습니다. 여기서 r÷b는 우리가 알고 싶은 1/b의 값의 근삿값을 얻기 위해 사용됩니다.

**예시로 설명**

가정: r=2^16 = 65536, b = 10이라고 해봅시다.

1. **r로 나누기**:
    - 2진법에서 65536으로 나누는 것은 매우 쉽습니다. 그냥 16비트를 오른쪽으로 시프트하면 됩니다. 예를 들어 12345678이란 숫자가 2진법으로 표현되었다면, 이 숫자를 65536으로 나누는 것은 비트를 16번 오른쪽으로 이동시키면 됩니다.
2. **r을 b로 나누기**:
    - r÷b=65536÷10=6553.6입니다. 이 값을 사용해 우리는 더 정밀한 나눗셈을 수행할 수 있습니다.
3. **결과 적용**:
    - 만약 2진법으로 표현된 수백만 자리의 숫자가 있다면, 이 숫자를 65536으로 나누고, 다시 6553.6으로 시프트하여 1/10의 근삿값을 구할 수 있습니다.

**결론**

설명된 방법은 매우 큰 수를 효율적으로 다루기 위한 고정밀 계산의 기법입니다. **r**이 2^k로 설정되면 2진법에서 **r**로 나누는 것이 시프트 연산으로 간단하게 이루어지기 때문에, 고정밀 계산에서 매우 유리합니다. 그 후에 **r을 b로 나눈 값**을 사용해 더 정밀한 결과를 얻습니다. 이 과정은 대규모의 계산을 효율적으로 처리하기 위한 방법입니다.

--- 

##### 고정밀 나눗셈의 동작과정
1. **초기설정**
	$r$을 매우 큰 값으로 설정한다. ex) $r$ = $2^{64}$ 또는 $r = 10^{64}$ 등..
	$r$을 매우 큰 값으로 설정한 이유는 정밀도를 높이기 위해서. 큰 $r$을 통해 매우 작은오차만 남기게 되어, 나눗셈 결과의 정밀도를 높일 수 있다. 

예를들어 십진법 숫자 72가 있을 때 10^k로 나누려면 10^2인 100으로 나눈다. 그러면 소수점을 오른쪽으로 두번 시프트 한 결과 즉, 0.72가 나온다.

$f'(x_i) = -1/x_i^2$이다.
해당 식을 $x_i$에 관한 식에 대입하면 아래의 결과를 얻을 수 있다.
![[첨부 파일 소스/Image File 1/Pasted image 20240903151936 1.png]]

결과적으로 $x_{i+1} = 2x_i - bx_i^2/R$
$x_{i+1}$을 계산하는데, 곱셈과 간단한 연산을 통해서 값을 도출해낼 수 있다.
그리고 계속해나가서 $x_{i+2}, \space x_{i+3}$등을 구해나갈 수 있다.

해당 **고정밀 나눗셈**을 전체적인 그림에 추가해보자.
우선 우리는 자릿수의 정확도를 충분히 크게 하기 위한 $i$를 알아야 한다.
예를 들어 $R/b = 2^{16}/5$라고 가정해보고, $R/b$의 값을 구하는 과정을 살펴보자.
![[첨부 파일 소스/Image File 1/Pasted image 20240903155757 1.png]]

초기값(Initial guess)은 $R$의 값을 나누기 쉬운 $2^{16}/4 = 2^{14}$로 설정한다.
>$x_0 = 2^{14} = 16384$

해당 초기값을 통해 다음 $x$의 추정값인 $x_1$을 구하기 위해서 위의 식에 값을 대입한다.
>$x_1 = 2(16384) - 5(16384)^2 / 65536 = 12288$ (1자릿수 정확도 `1`)
>$x_2 = 2(12288) - 5(12288)^2 / 65536 = 13056$ (2자릿수 정확도`13`)
>$x_3 = 2(13056) - 5(13056)^2 / 65536 = 13107$ (4자릿수 정확도 `1310`)

제곱근을 계산할 때 이차적 비율로 수렴했던 것이 나눗셈에 뉴턴 방법을 적용할 때도 같다.

마지막으로 이것이 실제 구현이 가능한지 살펴보자

##### 나눗셈에 필요한 반복 횟수
- 나눗셈의 계산은 이차적 수렴이므로 자릿수는 각 단계마다 두 배가 된다.
- 따라서 $d$ 자릿수의 정확도는 $log \space d$ 번의 반복이 필요하다.
곱셉의 관한 알고리즘에 대해 $\theta (n^a), \space a >= 1$ 이라고 가정하자.
`(카라추바, Toom3 등 어떤 알고리즘을 사용하느냐에 따라 a 값은 달라질 것이다.)`
$n$ 자릿수에 대해서 필요한 총 곱셈은 $log \space n$번 이다.
	따라서 나눗셈은 $O(log \space n \space n^a)$
매 반복마다 곱셈은 $n^a$복잡도의 상한을 가지게 되므로 빅 $O$로 표기.

숫자들의 크기에 대해서 나눗셈의 복잡도를 개선할 수 있는 방법.
- 첫 번째 자리부터 **상수로** 해당 자릿수를 계산한다.
- 작은 자리 수부터 시작해서 $d$ 자릿수까지 늘려나간다.
$$c 1^a + c2^a + c 4^a \space ... \space c (d/4)^a + c  (d/2)^a + c  d^a \space < 2c \cdot d^a$$
해당 식의 상한선은 $2c\space d^a$이므로 결과적으로 = $\theta (n^a)$
코드를 수정하지 않고도, $O(log \space n \space n^a)$보다 더 나은 복잡도를 얻어낼 수 있다.

결과적으로 **나눗셈의 복잡도 = 곱셈의 복잡도** 가 성립된다.

#### (최종) 제곱근 계산의 복잡도
**제곱근 계산을 위해 무엇을 했는가 ?**
- $\sqrt a$를 계산하고자 했다.
- $\sqrt a$ 를 일단 $10^{2_d} \cdot a$로 만들고, 뉴턴법을 사용한다.
	여기서 사용한 뉴턴법은 $x_{i+1} = (x_i + a / x_i)/2$ 의 형태.
	특정 $x_i$에 대해서 이것을 계산할 때마다 나눗셈의 계산이 필요.
	나눗셈을 계산하는 것은 뉴턴 방법이고, 이것은 $2 x_i - b x_i^2/R$이 된다.
	위의 식에서 로그 함수만큼 반복 계산을 하고, 작은수가 커져서 큰 수가 되기 때문에 결과적으로 **나눗셈의 복잡도는 곱셈의 복잡도와 같다.**

**결론**
- **나눗셈의 복잡도 $=$ 곱셈의 복잡도**
- 제곱근 계산의 복잡도로 $\theta (n^a)$을 얻을 수 있다. 
	`어떻게 ?` => $n^a$번의 연산을 상수만큼 반복하여 $n$자릿수의 정확도를 가지게 된다.


## 5. 그래프 - 루빅 큐브
### DFS vs BFS 비교

| 특성         | DFS                  | BFS                        |
| ---------- | -------------------- | -------------------------- |
| **탐색 방식**  | 깊이 우선, 한 경로를 끝까지 탐색  | 너비 우선, 시작 정점에서 가까운 정점부터 탐색 |
| **구현**     | 스택(재귀)               | 큐                          |
| **최단 경로**  | 최단 경로 보장 X           | 최단 경로 보장 O (무가중치 그래프)      |
| **메모리 사용** | 적음 (재귀 깊이에 따라 다름)    | 많음 (많은 노드를 큐에 저장)          |
| **응용**     | 경로 탐색, 사이클 탐지, 위상 정렬 | 최단 경로 탐색, 레벨 탐색            |

**결과적으로**
**DFS :** 그래프가 깊고 경로를 찾고자 할 때 적합
**BFS :** 최단 경로 탐색이 필요한 경우에 적합

### 5.1 그래프 1 : 너비 우선 탐색(BFS)
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec13 1.pdf]]
####
---


>**그래프 탐색이란 ?**  그래프를 탐험하는 것과 같다.
### 5.2 그래프 2 : 깊이 우선 탐색 (BFS)
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec14 1.pdf]]
####
---


#### 복습
지난 시간엔 **BFS탐색**에 대해서 알아보았다. BFS는 기본적으로 시작점 S와 노드 사이의 가장 빠른 길을 알려준다.
그냥 그래프를 탐색하는 것과 달리 BFS의 장점은, S에서 도달할 수 없는 정점이 있는 경우 일반적인 탐색으로는 최단 경로가 무한으로 나올 테고 도달할 수 없게 된다. 하지만 BFS는 S에서 도달할 수 없는 정점을 알려준다.


#### DFS (Depth - First Search)
DFS는 헨젤과 그레텔에 나오는 이야기처럼 과자 부스러기를 흘리며 이동하는 것과 같다.
가장 깊은 깊이에 도달하여 막다른 길일 때, 뒤로 돌아가기 위해서 과자 부스러기를 보고 왔던 길을 찾아가야 하기 때문에. DFS는 이를 부모 포인터를 통해 되돌아갈 수 있다. 또는 해당 경로에서 이동하지 못하면 해당 반복문은 끝내버리고, 재귀적으로 이전으로 돌아가, 이전에 시행하지 않았던 경로로 이동한다.

**강하게 연결된 그래프**
- 그래프의 모든 정점 쌍 사이에 경로가 존재한다는 걸 의미. (방향 그래프만 해당)
**약하게 연결된 그래프**
- 모든 정점  쌍 사이에 경로가 존재하지만, 방향을 무시하고 무방향 그래프처럼 다뤄졌을 때 연결 되어있는 경우를 의미.



##### 간선(Edge)의 종류
![[첨부 파일 소스/Image File 1/Pasted image 20240904001940 1.png]]
**트리 간선(tree edge)**
- 방문했을 때, 방문하지 않은 곳으로 인도하는 간선.
- 트리 간선을 통해 새로운 정점을 방문하게 된다.
- **구별 방법 :** 부모 구조가 어떤 간선이 트리 간선인지 알려준다.

**순방향 간선(forward edge)**
- 정점(node)에서 자손으로 가는 간선
- **구별 방법 :** 정점을 방문하면 방문한 시간을 기록하여 구별(?)

**역방향 간선(backward edge)**
- 정점에서 조상으로 가는 간선
- **구별 방법** : 재귀가 시작할 때, 선조를 체크해두고 재귀가 끝나고 체크를 지우게 되면 어떤 노드가 선조인지 구별할 수 있다.

**교차 간선(cross edges)**
- 선조와 조상 관계가 아니고, 형제 관계에 놓여있는 정점 사이에 놓여있는 간선
- **구별 방법** : 다른 모든 간선에 해당하지 않으면 교차 간선이다.

##### 무방향 그래프에는 어떤 간선이 존재할까 ?
**트리 간선**과 **역방향 간선**만 존재가 가능하다.

노드가 여러개 있을 때, 모든 노드를 방문하게 되면 노드를 방문하는 간선은 **트리 간선**이 되고, 모든 방문을 마치고 마지막 노드에서 시작 노드로 되돌아오는 노드는 **역방향 간선**이 된다.

##### 왜 간선 분류에 신경을 써야 하는가 ?
**두 가지 문제**를 푸는데 도움이 된다.

###### 1. 순환 검출
그래프에 방향성 순환이 있는지 확인
그래프를 탐색하면서 **역방향 간선**이 있으면 순환 가능.

###### 2. 위상 정렬
**DAG(Directed Acyclic Graph)** 사이클이 없는 방향 그래프 즉, 방향 비순환 그래프에서 정점을 선형으로 정렬하는 기법.

**위상 정렬의 특징**
1. **DAG**에서만 가능하다.
2. 여러개의 올바른 위상 정렬이 존재할 수 있다.

**구현 방법**
**BFS**
1. 각 정점의 진입 차수(in-degree)를 계산한다.
2. 진입 차수가 0인 정점을 큐에 넣는다
3. 큐에서 정점을 하나씩 꺼내면서, 해당 정점과 연결된 간선을 제거하고, 연결된 정점의 진입 차수를 1 감소시킨다.
4. 진입 차수가 0이 된 정점은 큐에 다시 추가한다.
5. 큐가 빌 때까지 위의 과정을 반복하며, 큐에서 꺼낸 순서가 위상 정렬 순서가 된다.

**DFS**
1. 각 정점을 방문하지 않은 상태로 표시하고, 방문할 때마다 방문표시를 한다.
2. DFS를 수행하며, 현재 정점에서 모든 인접 정점으로 탐색한다.
3. 더 이상 갈 곳이 없으면 해당 정점을 스택에 넣는다.
4. DFS가 종료된 후, 스택에서 정점을 하나씩 꺼내면 그것이 위상 정렬 순서가 된다.

## 6. Caltech-MIT 최단경로
### 6.1 최단 경로 1 : 도입
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec15 1.pdf]]
####
---


#### 이번 강의의 목표
특정한 알고리즘을 배우지는 않을 것이지만, 일반적으로 대부분의 최단 경로 알고리즘이 특정한 경우의 문제를 해결하기 위해서 어떠한 방법으로 접근하는지 알아볼 것이다.

그리고 **최적해**와 **최적 부분 구조 조건**이라고 불리는 상당히 중요한 성질에 대해서 이야기 할 것이다. 이는 대부분의 **최단 경로 알고리즘**이 효율적인 복잡도를 얻기 위해 이용하는 기법이다. 당연하지만 점근적인 복잡도가 중요하다.
그를 얻기 위해 사용하는 도구가 **최적 부분 구조**. 

다음 강의에서 **다익스트라**, **벨만-포드** 총 두 개의 알고리즘을 배울 것이다.

**다익스트라 알고리즘**
- 간선에 가중치가 있을 때 최소 비용을 지불하는 경로를 찾아내는 알고리즘
- 가중치가 양수인 경우에만 가능
- $O(V \space log^V + E)$의 복잡도 = $O(V log^V + V^2)$

**벨만-포드 알고리즘** 
- 다익스트라와 똑같이 최소 비용 지불하는 경로 찾아내는 알고리즘.
- 가중치에 음수가 포함되어 있어도 가능
- $O(V+E)$ 의 복잡도.= $O(V^3)$

**경로는 지수적인 수만큼 존재한다.**

**$E$(간선의 개수) 가 $V^2$으로 표현되는 이유.**
- 일반적인 단순 그래프는 두 정점 사이에 최대 하나의 간선이 있지만, 다중 그래프의 경우에는 두 정점 사이에 여러 간선이 존재할 수 있다.

**벨만-포드의 복잡도는 왜이렇게 높을까 ?**
- 간선의 가중치에 음수가 추가되게 된다면, 최종적으로 음의 가중치를 가지는 순환을 찾아야 한다. 해당 구간을 순환하게 되면 전체적인 비용이 감소되기 때문이다.  이런 종류의 문제가 알고리즘을 더 복잡하게 만든다.

두 알고리즘의 복잡도는 모두 정점과 간선의 개수에만 영향을 받고, 가중치에는 영향을 받지 않는다. 가중치의 범위는 동적이기 때문에, BFS와 DFS로는 가중치가 있는 그래프의 최단 경로를 찾을 수 없다.

**그래프의 음의 사이클이 있는 경우**
한 번 순환할 때 마다 비용이 감소하는 사이클이 있다면, 해당 사이클을 무한으로 돌아 정점의 비용을 정의하기 어려워진다. 종료조건을 갖기 위해서는, 이런 음의 사이클과 관련이 있는 정점에는 미결정 또는 음의 무한대로 표시하고, 유한의 값을 갖는 정점들만 비용을 정의해야 한다.

**최단 경로 알고리즘의 기본적인 구조**
`간선 집합 예시 [u, v] u에서 v로 가는 간선을 의미`
간선 집합에 있는 모든 u(정점)에 대하여, `d[v]`(해당 정점까진의 비용)를 무한대로 초기화한다. 그리고 선행자를 NIL로 설정하고, `d[s]`(시작 지점까지의 비용)를 0으로 초기화한다. 만일 정점(v)까지의 비용보다 더 적은 비용의 경로를 찾았다면 `d[v]`의 비용을 갱신한다. 이 과정을 간선 (u, v)의 완화 과정이 라고 한다.

**최적 부분 구조 증명의 두 가지 정리**
>최단 경로의 부분 경로는 해당 부분 경로 까지의 최단 경로가 된다.

>**삼각 부등식**
>삼각 부등식은 세 점 $A, B, C$가 있을 때, 두 점간의 거리를 나타내는 함수 $d$가 다음과 같은 관계를 만족하는 것이다.
>$$d(A, C) <= d(A,B) + d(B+C)$$
>이 부등식은 **"직접 연결된 경로가 항상 더 길지 않다"** 라는 의미를 가진다.
>다익스트라 알고리즘에서 삼각 부등식이 성립되는 이유는, 이미 정점 별 최단 거리의 업데이트가 끝난 시점이기 때문에, 해당 정점의 거리(비용)은 다른 곳을 돌아가는 것보다 같거나 적을 수 밖에 없다.
>삼각 부등식이 성립되지 않는다면, 잘못된 그래프.
>
>삼각 부등식에서 $d(A,C)$는 $A \space -> C$ 의 **단일 간선의 가중치**를 의미하는 것이 아니라, **A에서 C로 가는 경로(path)의 비용**을 의미하는 것이다.

### 6.2 최단 경로 2 : 다익스트라
#### 강의 자료
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec16 1.pdf]]

#### 완화 단계
$(u, v, w)$에 대한 완화
만약 $d[v] > d[u] + w(u, v)$ 이면
- $d[v] = d[u] + w(u, v)$
- $\pi [v] = u$ ($v$의 선행자를 $u$로 설정)

##### 위의 완화 단계를 증명하는 보조 정리
- 모든 정점에 대해 $d[v]$가 $\delta (s,v)$보다 크거나 같다는 사실은 변하지 않는다.

##### 보조 정리 증명
과정의 횟수에 대해 수학적 귀납법 이용
$d[u]$가 $\delta (s,u)$보다 크거나 같을 것이라고 가정.
삼각 부등식에 의해

#### DAG
비순환 방향 그래프의 약자. (비순환이므로 음의 순환을 가질 수 없다.)
음의 간선이 있다고 해도, 음의 순환이 아니면 크게 문제되지 않는다.
1) 최단 경로를 구하기 위해 먼저 DAG를 위상정렬한다. $u$에서 $v$까지의 경로라는 것은 $u$가 $v$이전에 온다는 것을 의미한다. 이 작업을 하게 되면 선형 순서를 갖게 된다.
2) 왼쪽에서 오른쪽으로 한 번 위상 정렬이 된 순서로 정점을 훑어준다. 그리고 각 정점에서 나가는 간선에 대하여 완화를 해준다. 

>**결과적으로** $O(V + E)$time 선형시간에 해결 가능.

#### 다익스트라
**다익스트라** 알고리즘은 **그리디 알고리즘**이기 때문에 상당히 직관적이다. 이익을 극대화하면서 반복적으로 최단 경로를 만든다.

특정한 연산에 대하여 어떤 것이 더 나을지 비교해보자.

##### 다익스트라의 의사 코드
![[첨부 파일 소스/Image File 1/Pasted image 20240909192832 1.png]]
![[첨부 파일 소스/Image File 1/Pasted image 20240909192842 1.png]]
$\theta (V)$만큼 **우선순위 큐**(priority Queue)에 삽입.
$\theta (V)$번 **EXTRACT-MIN** 연산 진행
정점은 한 번씩만 삭제되고 처리되기 때문에, 정확히 $\theta(V)$만큼의 연산이 이루어진다.
그리고 $\theta(E)$만큼의 키 감소 혹은 갱신 연산이 있다.
왜나하면 완화를 진행할 때 키를 감소시키는 연산이 되기 때문이다.(d값을 줄이기 때문에)
$\theta (E)$만큼인 이유는 방향 그래프에서 정점에서 나오는 간선들을 딱 한 번만 처리하기 때문이다. 그 정점의 모든 진출 간선에 대해 처리하므로.

>그래서 우선순위 큐를 어떻게 구현해야 할까 ?

##### 배열로 구현했을 때 다익스트라의 복잡도
**EXTRACT-MIN의 복잡도 :** $\theta (V)$
**키 감소의 복잡도 :** $\theta (1)$
 >결과적으로 $\theta (V \cdot V + E \cdot 1) = \theta(V^2)$ ($E$는 $V^2$의 복잡도를 가진다.)

##### 이진 최소 힙으로 구현했을 때 다익스트라의 복잡도
**EXTRACT-MIN의 복잡도 :** $\theta(log \space V)$
**키 감소의 복잡도 :** $\theta(log \space V)$
> 결과적으로 $\theta (V \cdot log \space V + E \cdot log \space V)$

하지만 이는 다익스트라 알고리즘의 최선의 복잡도가 아니다.
**피보나치 힙**이라는 특정한 자료구조를 사용하면 복잡도를 줄일 수 있다.

##### 피보나치 힙으로 구현한 다익스트라의 복잡도
*피보나치 힙은 분할 상환 자료 구조이다.*
**EXTRACT-MIN :** $\theta(log \space V)$
**키 감소 :** $\theta (1)$ 분할 상환 시간만에 가능하다.
> 결과적으로 $\theta (V \cdot log \space V + E)$time으로 해결 가능하다.

#### 결론
다익스트라는 분할 상환과 적절한 자료 구조(피보나치 힙)을 이용해 선형 시간에 해결 가능하다.

---

### 6.3 최단 경로 3 : 벨만-포드
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec17.pdf]]



#### 일반적인 최단 경로 알고리즘 복습
**일반적으로 최단 경로를 찾는 알고리즘의 과정**
그래프에 포함된 모든 정점이 초기에 무한대의 최단 경로 가중치를 가지고 있다고 설정.
이전 정점들의 값은 비워두고, 시작점 $s$의 $d[s]$의 값을 0으로 설정.
알고리즘의 중심 루프에서 반복해서 간선을 선택.
**간선을 선택하는 특정한 방법**
- 양의 가중치를 가진 간선 중 가장 우선순위에 있는 간선을 선택.
**선택한 간선으로 완화 단계를 거친다.**
1) Relax edge $(u, v, w)$ $d[v]$의 값이 $d[u] + w$의 값보다 크면 $d[v]$의 값을 업데이트 한다.
2) 이전 정점을 가리키는 포인터를 정확하게 설정한다.
해당 완화 과정을 더 완화할 수 없을 때까지 반복한다.

##### 해당 과정이 가지고 있는 두 가지 문제
1. **시간 복잡도에 관한 문제**
	양의 가중치를 가지고 있는 간선도 지수 시간 복잡도를 가진다.
	지수 개수의 경로를 가지고 있고, 일직선으로 위상정렬한 모양의 그래프가 주어진다면, 병적으로 많은 간선 선택이 완화를 지수 횟수만큼 시행한다.
	해당 그래프가 n개의 정점을 가지고 있다면 $O(2^{(n/2)})$time 을 가진다.

2. **알고리즘의 종료 조건이 명확하지 않다.**
	음의 가중치를 가진 순환이 시작점으로부터 도달 가능하다면, 알고리즘은 정해진 대로 종료하지 않는다.

**첫 번째 문제 해결 방안**
간선이 양의 가중치만 존재할 때, **다익스트라 알고리즘**을 이용해 해결 가능하다.

**두 번째 문제를 어떻게 해결할까 ?**
음의 가중치를 가진 그래프가 비순환 방향 그래프의 경우는 문제가 없지만, 음의 순환이 있는 경우를 처리하기 어렵다.

>다항 시간 알고리즘은 good, 지수 시간 알고리즘은 bad, 무한 시간 알고리즘은 fire.

#### 벨만-포드 알고리즘
음의 가중치 간선이 있는 그래프의 최단 경로를 찾는 알고리즘.
만약 그래프에 음의 순환이 있다면, 음의 순환을 찾아내고 최단 경로 계산을 멈춘다.

**알고리즘의 기본 가정**
시작 정점 $u$ 에서 $u$까지의 최단 경로의 가중치는 0으로 두고, 나머지 정점들과의 최단 경로의 가중치는 무한대로 둔다. 그중 일부는 무한대의 값을 그대로 유지하고, 또 다른 일부는 유한한 최단 경로의 가중치 값을 얻게 된다. 음의 순환을 가지고 있는 그래프라면 일부의 경로 가중치는 정의되지 않는다`(undefinded)`

**코드 전개 예시(python)**
![[첨부 파일 소스/Image File 1/Pasted image 20240909224424 1.png]]
```
Bellman Ford(\delta, w, s)
	Initialize() // 초기 설정
	for i = 1 to |V| - 1
		for each edge (u, v) (- E
			Relax(u, v, w)

	// 음의 순환 확인 과정
	for each edge (u, v) (- E
		if d[v] > d[u] + w(u, v)
			then report -ve cycle exists

Relax(u, v, w)
	if d[v] < d[u] + w(u, v) :
		d[v] = d[u] + w(u, v)
		ㅠ[v] = u
```
만일 그래프에 음의 순환이 없다는 것을 미리 알고 있다면, $V - 1$번의 실행 이후에 정확한 최단 경로를 찾을 수 있다. (뒤에서 증명)

전체 시간 복잡도는 $O(V\cdot E)$
$E$는 $O(V^2)$의 시간 복잡도.
결과적으로 벨만-포드 알고리즘은 $O(V^3)$의 시간 복잡도를 가질 수 있다.

다익스트라 알고리즘은 거의 선형시간에 해결할 수 있기 때문에 더욱 효율적이지만, 음의 가중치를 고려해야 하는 상황인 경우에는 어쩔 수 없이 벨만-포드 알고리즘을 사용해야 한다.

##### 증명이 필요한 두 가지
1) 벨만-포드 알고리즘의 실행을 마치고 났을 때,  음의 순환이 없다는 가정 하에 $d[v] = \delta(s, v)$ 가 성립된다.
2) 만일 $d[v]$의 값이 $v-1$번의 계산 이후에도 수렴하지 못한다면, 정점 s로부터 음의 순환에 도달할 수 있다.

먼저 생각해야 할 부분은 일반적으로 최단 경로가 의미하는 바를 생각해야 한다.
시작점 s가 있고, 특정한 정점 v가 있을 때 이 정리를 증명하기 위해 염두에 두어야 할 그림이 있다. 
![[첨부 파일 소스/Image File 1/Pasted image 20240909225623 1.png]]
해당 그래프에서, 경로 $P: V_0, V_1, ... V_k$
이 때 $K$의 값은 $V - 1$보다 작거나 같다.
만일 $K$의 값이 $V - 1$보다 커진다면, 해당 그래프는 순환을 가지게 된다.

**해당 정리를 귀납법을 이용해서 증명할 것이다.** 
`(수학적 귀납법 : 가정이 모든 경우에 대해 성립합을 증명. 증명 과정이 타당하다면 결론 역시 타당하다.)`
>가정$$v \in V , \space p = <v_0, v_1, v_2 ... v_k> , \space v_0 = S \space \space to \space \space v_k = v$$

경로 $p$는 최단 경로이지만, 유일한 최단경로는 아니며, 최소 개수의 간선을 가진 경로가 아니라고 전제. 따라서 최단 경로가 여러개 존재할 수 있다.

여러개의 최단 경로 중, 간선이 가장 적은 경로를 고른다.
음의 순환이 없다면$ p$는 단순 경로를 의미한다.
$p$가 단순 경로라면 $k <= v-1$ 성립.

##### 전체적인 과정
$v$를 임의의 정점으로 가정.
$d[v]$는 정점에서 $v$까지의 가중치를 저장하는 list이고,
$\delta [s, v]$는 정점 s에서 $v$까지의 최단 거리를 의미한다.
1) 정점 $s$인 $v_0$에서 $v_1$까지의 간선$(v_0, v_1)$에 대해서 완화를 진행한다.
2) 1의 과정을 통헤  $\delta (s, v_0)$의 값을 얻을 수 있다.
3) 1,2의 과정을 한 단계씩 $v_k$까지 반복해나가며 각 정점까지의 최단 경로를 구한다.
4) 총 $k$번 계산하고 나면 $v_k$까지의 최단 경로를 구할 수 있다.

>**결론 :** $v -1$ 번 실행하면 $s$로부터 도달가능한 모든 정점에 대해서 최단 경로 값을 구할 수 있다.

>만일 $v-1$번의 알고리즘 실행 후에도 완화 가능 간선이 존재한다면 ?

이는 s에서 도달 가능한 어떤 정점까지의 현재 최단경로가 단순 경로가 아니란 것을 의미한다. 해당 간선을 완화할 수 있으면 반복되는 정점을 가지기 때문.
**즉, 순환을 찾은 것을 의미하며 이는 음의 가중치를 가지는 음의 순환이다.**

##### 최장 단순 경로와 최단 단순 경로
- **양의 가중치를 가지는 그래프의 최장 단순 경로 찾기 :** 각 간선을 음수화하고 벨만-포드 알고리즘을 이용한다고 생각할 수 있지만, 그래프가 순환을 가지는 경우 벨만-포드 알고리즘은 실행을 종료하므로 계산하지 못한다. **NP-hard 문제**

- **음의 순환이 있는 그래프에서 최단 단순 경로를 찾기 :** 이는 다항 시간의 알고리즘으로 해결할 수 없고, 반드시 **지수 시간 알고리즘**으로만 해결할 수 있는 **NP-hard** 문제에 속한다. 같은 조건에서 **최장 단순 경로 찾기** 또한 **NP-hard**에 속한다.

#### 결정 문제의 분류
1. **P와 NP**
- **P (Polynomial time) :** 주어진 문제를 결정론적 알고리즘으로 다항시간 (O(n^k))안에 풀 수 있는 문제들. 즉, 컴퓨터가 짧은 시간 안에 답을 구할 수 있는 문제들을 의미한다.
- **NP (Non-deterministic Polynomial time) :** 주어진 문제를 해결하는 데에는 오랜 시간이 걸리지만, 문제의 답이 맞는지는 다항 시간 내에 확인할 수 있다.

2. **NP-hard**
- **NP-hard :** NP에 속하는 문제들을 포함해 더 어려운 문제들을 의미한다. 모든 NP-hard 문제가 그런 것은 아니지만, 답을 검증하는 시간조차 다항 시간을 벗어나는 문제들이 포함된다. (예: 최적화 문제) 

### 6.4 최단 경로 4 : 다익스트라 가속화
#### 강의 자료
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec18.pdf]]

####
해당 강의는 **다익스트라 알고리즘**의 성능 개선에 대해 다룬다.

이전 최단경로를 구하는 것은, 암묵적으로 단일 시작점에서 단일 도착점 또는 모든 도착점의 최단경로를 구하는 것으로 가정했다. 다른 경우의 문제에 대해서도 살펴보자.

단일 시작 단일 도착 문제
시작 지점을 모르고 임의의 점 s에서 t까지의 최단 경로를 찾는 문제에 대해 실행시간을 줄이는 코드 최적화를 해보자. (최악의 시간 복잡도는 변함이 없으며 대부분의 경우에 성능 향상을 기대할 수 있다.) 

#### 양방향 탐색
##### 양방향 탐색의 핵심 아이디어
시작지점에서 출발하는 탐색과 도착 지점에서 출발하는 탐색을 동시에 진행해, 두 탐색이 중간에서 만나면 종료한다. 이를 통해 탐색 공간을 줄이고, 탐색 속도를 크게 향상시킬 수 있다.

양방향 탐색에는 정점 s와 t가 있고, 그 사이에 그래프를 나타내는 정점들이 있다. 순방향과 역방향을 번갈아가며 탐색한다.
1) **전방향 탐색의 첫 단계**
- 먼저 s에서 시작하는 기존 다익스트라 알고리즘의 첫 번째 단계를 실행해서 순방향으로 경로를 찾는다.
- 즉 첫 번째 단계에서 순방향으로의 경로를 탐색하기 위해 Q에서 정점 s를 뽑아내고 s의 진출 간선에 대해 계산한다.  
2) **전방향 탐색을 멈추고 역방향 탐색을 한 단계 실행한다.**
- t에서부터의 역방향 탐색이란 ? -> 간선을 반대로 따라가는 것
- 즉, 그래프의 데이터 구조는 간선을 순방향으로도, 역방향으로도 참조할 수 있어야 한다.
- 역방향 단계에서 첫 번째 경계인 t가 우선순위가 가장 높은 항목이 된다.

이제 양방향 탐색을 하기 위한 두 개의 우선순위가 필요하다.
순방향에서는 ? $v[s]$는 0이므로 $d_f[s] = 0$
역방향에서는 $d_b[t] = 0$ 이다.
$d_f[\space ]$ 와 $d_b[\space ]$는 각 순방향과 역방향에서의 우선순위를 가리키며, 상호 보완적이다.
서로 다른 우선순위인 $Q_f$와 $Q_b$또한 필요하다.
처음 시작할 때 순방향에서는 $d_f[s]$를 제외한 $d_f[\space]$의값을 모두 무한대로 설정하고, 역방향에서는 $d_b[t]$를 제외한 $d_b[\space]$의값을 모두 무한대 로 설정한다.

>Question : 종료 조건은 무엇인가 ?

- 어떤 정점 u가 전방향 탐색과 역방향 탐색 모두에서 처리된 경우. (경계가 만난다는 것을 의미)

>종료 후에 s에서 t까지의 최단 경로를 어떻게 찾을까 ?

- 자료구조 $\pi_f$ 와 $\pi_b$ 가 있어야 한다. $\pi_f$ 는 기존과 동일하고 $\pi_b$ 는 간선의 방향과 반대로 가리킨다.

![[첨부 파일 소스/Image File 1/Pasted image 20240912202923.png]]

위의 그림에서 $v_2 -> t$를 경로로 선택하는 경우 $t$로 도달하는 최단 경로에 해당 간선이 포함된다.($t$에 도달하는 유일한 간선이기 때문)
즉, $\pi_b[v_2] = t$ 이고 $\pi_f[v_1] = s$ 이다.

##### 양방향 탐색의 종료 조건
어떤 정점 $w$가 계산되었을 때 종료된다. 즉, 순방향 역방향 모두의 우선순위 큐 $Q_f$ 와 $Q_b$ 에서 $w$가 제거되었을 때 종료한다.

**최단 경로 구하기**
- 순방향 :  $\pi_f[\space]$를 통해 $s$부터 $w$까지 이동하면서 가중치의 합을 통해 거리를 구한다. 
- 역방향 :  $\pi_b[\space]$를 통해 $t$부터 $w$까지 이동하면서 가중치의 합을 통해 거리를 구한다.
but 종료 조건 자체는 맞지만, $w$가 최단 경로에 없을 가능성 존재가 있으며,
이렇게 되면 해당 경로는 최단 경로가 아니게 된다.

**$w$가 최단 경로에 속해있지 않을 수 있음을 입증하는 과정**
순방향과 역방향을 한 번씩 탐색하면서 각 누적된 가중치의 크기에 따라 다음 우선순위를 정하여 탐색한다.
**그림을 통한 예시**
![[첨부 파일 소스/Image File 1/Pasted image 20240913003201.png]]


순방향과 역방향 모두 정점 $w$에서 경계가 형성되는데, $w$는 최단 경로에 있는 정점이 아니다.

**최단 경로에 속한 경계지점을 어떻게 찾을까 ?** 
문제를 해결하기 위해 경로를 구하는 과정에 몇가지 작업을 추가해야 한다.
순방향 경계에 있는 모든 노드를 확인해서 그 이웃이 역방향 경계에 있는지 확인하면 이전에 꺼낸 정점보다 더 짧은 경로를 구할 수 있는지 확인한다.

**증명**
$w$와 다를 수 있는 정점 $x$를 찾되 $d_f[x] \space + \space d_b[x]$ 의 값이 최소인 정점을 찾는다.
해당 정점 $x$를 위의 순방향, 역방향의 최단 경로를 구하는 식에 $w$대신 $x$를 대입한다.

**전체 과정 정리**
교대로 탐색하고, 탐색을 종료하고 적절한 $x$를 찾아서 최단 경로의 길이를 최소화 할 수 있는지 확인한다.

#### 휴리스틱
문제 해결에서 빠르고 간편하게 결과를 얻기 위해 사용하는 접근 방식.
복잡한 문제를 다룰 때 완벽한 해법을 찾기보다는 시간이나 자원 제약 내에서 현실적인 해법을 제시하는 방법이다.
휴리스틱은 정확한 알고리즘과 달리, 항상 최적의 해답을 보장하지는 않지만, 실제 상황에서 효율적으로 적용될 수 있다는 장점이 있다. `(대표적인 예로는 Greedy algoritm)`

간단하게 말하자면, **실제에서 더 빨리 실행되도록 그래프를 수정하기 위해 사용하는 방법**
`ex)목표 지향 탐색`

##### 목표 지향 탐색 또는 A* 알고리즘
정점에 대해 잠재력 함수에 따라 간선의 가중치를 수정하며 최단 경로를 찾아가는 것.
그러기 위해서 우선 순위가  휴리스틱하게 수정되며, 결과적으로 실행 속도가 빨라진다.

**간선의 가중치를 수정하는 방법** ![[첨부 파일 소스/Image File 1/Pasted image 20240912235021.png]]
$\lambda$ 라고 불리는 일종의 잠재력 함수를 이용한다.
만약 $u$와 $v$사이에 간선이 있다면 주어진 $\lambda$ 함수에 대해서 위의 식을 만족한다.

**정확성 확인** ![[첨부 파일 소스/Image File 1/Pasted image 20240913000537.png]]
휴리스틱을 통해 구한 최단경로를 기존 최단경로에서 $\lambda$`(잠재력)` 함수에 $s$를 넣고 빼주고, $t$를 넣고 더해준다.(시프팅 과정) 그렇게되면 기존의 최단 경로와 정확히 일치한다.

$\lambda$ `(잠재력)`함수는 랜드마크를 활용해 구할 수 있다.

##### 랜드마크
랜드마크는 그래프에서 중요한 참고 지점을 의미하며, 잠재력 함수의 정확도를 높이기 위해 사용된다. 출발지와 목적지 사이에 몇몇 고정된 랜드마크를 설정하고, 이들간의 거리를 사전에 계산하여 휴리스틱으로 활용한다.


![[첨부 파일 소스/Image File 1/Pasted image 20240913001128.png]]
그래프의 정점 중 하나를 랜드마크 $l$이라 두고, $\delta (u, l)$ 을 계산한다.
$\delta (u, l)$ 이 의미하는 바는 어떤 정점을 입력받던, 랜드마크 $l$ 까지의 최단 경로를 구하는 것이다.
>$\lambda (t, u) \space = \space \delta(u, l) \space - \space \delta(t, l)$ `람다 함수는 왼쪽 식과 같이 정의된다.`

**동작 과정**
1. 그래프에 속하는 모든 정점 $u$에 대해서 $\delta(u, l)$을 사전 계산한다.
2. 주어진 t에 대해서 $\delta(t, l)$도 계산한다. `(t는 하나의 정점으로 한 번만 계산하면 된다.)`
3. 해당 잠재력 함수 $\lambda$를 이용하면 **삼각 부등식**을 이용해서 정확성을 검증할 수 있다.

랜드마크를 설정함으로써 다익스트라 알고리즘은 20% - 2배의 속도 향상을 기대할 수 있다.

>여담- 강사의 말하기 화법. 학생이 무언가 아주 기본적인 것을 질문했을 때 조차, 그럴 경우는 없다 라거나 그런 경우에는 이런 상황이 아니니까 아니겠죠 ? 와 같이 이미 해당 논리가 정해져있다고 가정하고 설명하는 것이 아니라 기본적인 해당 논리에 대해서 다시 설명해줌으로써 가정 - 결론 이 완벽하게 이뤄진다.



## 7. 동적 - 이미지 압축
### 7.1 동적 프로그래밍 1 : 메모이제이션, 피보나치 수, 최단경로, 추측
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec19.pdf]]

####

**Dynamic Programming의 어원**
영국인들에게 Programming이란 최적화(Optimization)을 의미한다.
DP는 리처드 벨먼(벨만-포드 알고리즘 창시자)가 개발했으며, 벨만-포드 알고리즘의 기초라고 할 수 있다.
리처드 벨먼이 DP를 개발할 당시 수학 연구에 대한 내용을 숨기기 위해 다소 쉬운 Dynamic Programming으로 이름을 지은 것이다.

**동적 프로그래밍은 최적화를 실행하는 방법 중 하나이다.**
무언가의 최소값이나 최대값을 계산하기 위한 알고리즘이지 마법이 아니다.


#### Dynamic Programming의 기본 개념
**주어진 문제를 하위 문제로 쪼갠 뒤 하위 문제를 풀고 그 해를 다시 사용하는 것.**
동적 프로그래밍의 가장 어려운 부분은, 하위 문제를 알아내는 것이다.
하위 문제는 실제 문제 풀이를 돕도록 설계되어야 한다.

#### 메모이제이션 DP 알고리즘
**메모이제이션**
하위 문제를 풀었다면, 해를 저장하고 나중에 같은 하위 문제를 해결할 때 해를 다시 사용하는 것이다. `ex)피보나치`
필요한 하위 문제의 수를 세고, 하위 문제는 한 번씩만 풀이하면 되므로 중복 계산을 방지하여, 중복으로 계산 될 경우를 고려하지 않아도 된다.

**DP를 통한 피보나치 수 계산 코드**
```
// 가장 높은 수의 피보나치 수를 계산하기 위하여 아래로 채워나간다.
let memory = [0,1,1]
function fib(n) {
	if (memory[n]) {
		return memo[n]
	}
	else {
		memo[n] = fib(n-1) + fib(n-2)
	}
	return memo[n]
}
```

결과적으로 **동적 프로그래밍**은 **재귀 호출**과 **메모이제이션**이 합쳐진 것.
**시간 복잡도** = 하위 문제의 수 $\cdot$ 하위문제 해결 시간
**동적 프로그래밍**에서는 재귀 문제를 풀 필요가 없다. **메모이제이션**을 통해 저장한 값을 꺼내오기만 하면 되기 때문.

위에서 피보나치를 계산한 코드는 가장 위에서부터 계산한 Top-down 형식이다. 경우와 관점에 따라 bottom-up 방식과 top-down 방식 중 편한 것을 사용하면 된다.

#### 상향식(bottom-up) DP알고리즘
**상향식 알고리즘의 피보나치 수 계산 코드**
```
// 가장 낮은 수의 피보나치 수부터 계산하여 채워나간다.
let fib = [0]
let f = 0
for (let i = 1; i <= n; i ++) {
	if (i <= 2) {
		f = 1
	}
	else {
		f = fib[i-1] + fib[i-2]
	}
	fib[i] = f
}
return memory[n]
```

- 해당 코드는 메모이제이션 알고리즘과 **완벽하게 똑같이** 작동한다. 단지 반복문과 재귀 함수의 차이일 뿐, 재귀 함수를 풀어서 나열한다면 해당 코드와 동일한 계산을 한다.
- 상향식 접근은 하위 문제 의존 DAG(방향성 비순환 그래프)의 **위상 정렬**. 
  `피보나치를 예시로 든 이미지`
![[첨부 파일 소스/Image File 1/Pasted image 20240915140327.png]]
왼쪽 -> 오른쪽으로 위상적 순서가 존재.

##### 상향식 접근의 장점
**공간 절약이 가능하다.**
- 필요한 부분만 저장하기 때문에

**실행 시간이 명확하게 보인다**
- 메모이제이션 방식은 하위 문제와 하위 문제의 시간을 곱해줘야 하지만, 상향식 접근은 코드에서 직관적으로 확인할 수 있다.


#### 최단 경로
**문제** 모든 정점 v로부터 s부터 v까지의 최단 경로 찾기

시작점 s에서 v까지의 최단 경로를 구할 때, 각 정점에서 v까지의 최단 경로를 구하고 정점 v로 가는 가장 좋은 마지막 간선을 선택한다.

**방법**
v를 종점으로 갖는 모든 간선을 시도해보고, 해당 간선을 u,v라고 추측한다.
s에서 u까지의 최단 경로를 재귀적으로 계산한다.
그 후 각 경로의 길이에 간선 u,v의 가중치를 추가하여 최단 경로를 구한다.
>**재귀 공식**
>$$\delta (s,v) = min (\space \delta(s,u) + w(u,v))$$

방향성 비순환 그래프에서는 $O(V+E)$의 시간이 걸린다.
- **하위 문제의 개수 X 하위 문제당 시간**
$\delta (s,v)$의 하위 문제에 걸리는 시간은 정점 $v$의 진입 차수이다. 즉, $v$로 가는 간선의 개수.
그러므로 모든 하위문제의 합은 모든 정점의 진입 차수의 합이다. = 총 간선의 개수
![[첨부 파일 소스/Image File 1/Pasted image 20240916185206.png]]

비순환 방향 그래프 **(DAG)** 에서 메모이제이션을 활용한 알고리즘은
벨만-포드 알고리즘의 한 단계를 실행하기 위해 깊이 우선 탐색 **(DFS)** 을 통한 위상 정렬을 하는 것이다.
즉, **BFS/위상정렬 + 벨만-포드 한 단계**가 하나의 재귀로 압축된 과정

**But,** 그래프에 순환이 있다면 실행 시간에 무한이 걸리게 된다.

>**순환이 있는 방향 그래프에서는 어떻게 실행해야 할까 ?**

**순환 그래프를 비순환 그래프로 바꾸기**![[첨부 파일 소스/Image File 1/Pasted image 20240915145733.png]]
- k개의 정점을 가진 그래프를 k개로 나눈다. 
- 간선을 따라가면 다음 층으로 이동한다.

>**k라는 변수를 추가해서** $$\delta_k(s,v) = min(\delta_{k-1}(s,u) + w(u,v))$$

우리가 구해야 할 부분은 $\delta_{v-1} (s,v)$ 이다. k의 범위는 0부터 v-1이고 총 하위 문제의 개수는 $v^2$개
모든 정점에 대해 진입 차수의 합을 구하므로 총 실행 시간은 $O(VE)$.
이는 벨만-포드 알고리즘의 실행시간과도 동일하다.
**벨만-포드** 알고리즘은 **동적 프로그래밍** 방식으로 접근한 것이다.


### 7.2 동적 프로그래밍 2
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec20.pdf]]
####

**DP $\approx$ careful brute force**
- 무차별적으로 대입하되, 기하급수적으로 많은 경우를 시도하는 것은 피한다.
- 문제에 대해 잘 생각해서 지수 탐색 공간을 다항 탐색 공간으로 줄일 수 있다.
**DP $\approx$ gusssing(추측) + recursion(재귀) + memoization(메모이제이션)**
- 추측 : 어떤 문제를 푸는 최고의 방법을 찾고 싶을 때 문제에서 알고 싶은 특징을 고른다.
**DP $\approx$ shortest paths in some DAG**
- DP는 항상 DAG(방향성 비순환 그래프)에서 최단 경로를 계산하는 것이 된다. 그러기 위해 풀어야 하는 문제를 DAG로 바꾼다. 그게 DP가 하는 일이고 새로운 개념이 아니다. 중요한 것은 **DAG를 얼마나 영리하게 만드는가 ?**


**5 easy step to DP(반드시 순차적인 단계는 아님)**
1) **하위 문제 정의**
	해당 단계에서 하위문제의 수가 얼마나 되는지 알아야 한다.
2) **해의 일부 추측하기**
	 문제를 풀고 싶을 때 답의 특징을 추측하는 것이다.
	 무언가를 추측해야 한다면, 선택지의 개수를 적어야 한다.
	 `추측에 몇 개의 선택지가 있을까 ?`
	 Ex.배낭 문제
		물건을 넣는다, 물건을 넣지 않는다 두 개의 행동을 추측한다. 그 중에서 물건을 넣는다면 다른 물건을 빼고 넣을지 선택지가 존재한다.
	Ex. 최단 거리 경로 문제
		s에서 v로 가는 최단 거리 문제에서 추측한 것은 v로 가는 마지막 간선을 추측했다. (u,v)로 가는 간선. 해당 간선에서 선택지는 v의 진입차수만큼 존재한다.
	Ex. 프로그래머스 동적 삼각형 문제
		각 위치에서 해당 위치의 값에 아래 위치한 값을 더해주는 것을 추측한다. 해당 추측에서 선택지는 왼쪽 아래 값, 오른쪽 아래 값 총 2개가 있다.
3) **하위 문제와 해 연관짓기 (점화식 세우기)**
	보통 추측의 개수와 하위 문제의 실행 시간은 같은 상수이다.
4) **알고리즘 만들기 (재귀 & 메모이제이션 or 상향식으로 DP table 만들기)**
	`상향식(bottom -up)방식은 재귀&메모이제이션을 간단한 for문으로  바꾸는 것이다.`
	해당 단계에서 하위 문제의 실행 시간을 계산해야 한다.
5) **원래의 문제 풀기**

**문제 풀기**
1) 하위 문제 정의
	남은 단어에 대해 고민하므로 하위 문제는 **suffixes**가 되고, 하위 문제의 개수는 n개이다.
2) 추측하기
	2번째 줄이 어디서 시작할까 ?라는 추측에 대한 선택지는 남은 단어에 대해 고민하므로 n - i개 = $O(n)$이다.
3) 재귀 관계(점화식 세우기)
	하위 문제의 이름을 DP(i)라고 정한다. i는 첫 번째 줄의 시작 단어이고, j는 두 번째 줄의 시작 단어이다. 두 번째 줄이 어디서 시작할지에 대해 i 이후의 모든 단어에 대해 for문을 돌려 비용을 계산한다. 강의 노트에 나오는 badness 지수를 계산하는 방법은 복잡하지만 그건 상관 없다. 함수로써 식에 포함시켜서 비용을 계산하면 된다. 이를 점화식으로 표현하자면 
	`for (let j = i + 1; j < n + 1; j ++)` = i 이후의 모든 j에 대해
	$DP(i) = min(DP(j) + badness(i,j))$
4) 위상 정렬
	위에서 세운 식을 끝에서부터 처음까지 거꾸로 계산해야 한다.
	`i = n, n-1, ..., 0`
	총 걸리는 시간은 하위 문제의 개수인 `n`에 하위 문제당 시간인 `O(n)`을 곱한 $\theta (n^2)$이다.
5) 원래 문제
	이 경우에 원래 문제는 DP(0)이다. 0번째 단어부터 선택하므로 모든 단어가 선택되원래 풀려던 문제가 된다.

해당 과정은 단어를 어떻게 나누는지를 알려주는 것이 아니라, 단어를 나누는 비용을 알려주는 것이다. 비용을 알고 나면 해당 비용이 가중치의 역할을 하므로, 최단 경로를 구하는 건 쉽다.
**Parent Pointers**
**ㄴ> 어떤 추측이 가장 좋았는지 기억하는 것.**
모든 동적 프로그램에 적용 가능하고, 해의 비용이 아닌 실제 해를 찾을 수 있게 해준다.

parent pointer를 활용하여, 각 줄의 시작 단어를 구해보자.
첫 줄은 0에서 시작할 것이므로 0, 그 다음은 0의 parent pointer 이렇게 진행될 것이다.
`0 -> parent[0] -> parent[parent[0]] -> ...`

Dynamic Programming은 자동화 되어 돌아가기 때문에, 어떤 걸 추측하는지와 하위 문제를 알아내는 것 이 두가지 부분(순서 상관 X)이 어려우며 두 가지를 알아낸 뒤의 과정은 쉽다.



### 7.3 동적 프로그래밍 3
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec21 (1) 1.pdf]]
####

이번 강의에서 다룰 세 가지 문제
1) **괄호 묶기 문제**
2) **DNA 문자열을 비교하는 것과 같은 편집 거리 문제**
3) **배낭 문제**
두 가지 일반적인 아이디어
문자열 문제를 어떻게 동적 프로그래밍으로 다룰 것인지.

**다이나믹 프로그래밍의 5가지 단계(순차적 X)**
**a)** 하위 문제 정의하기
**b)** 추측하기 (해법의 일부분을)
**c)** 하위 문제의 풀이 연관짓기 (점화식 세우기)
**d)** DP를 재귀적 or 메모이제이션을 통해 알고리즘 구현하기 (비순환적인지 파악or 위상학적 순서 파악)
**e)** 원래 문제를 풀 수 있는지 확인하기 (하위 문제의 유형에 속하거나 여러가지의 조합으로 이루어져있는지 확인)

DP에서 가장 어려운 부분은 1단계일 것이다. (define subproblem)
>1단계에서 하위 문제를 잘 고를 수 있는 방법들은 어떤 것들이 있을까 ?

**Suffixes**
`모든 i에 대하여` `x[i:]` 
>**suffixes란 ?**
>배열이나 문자열에서 특정 위치에서 끝까지의 부분을 의미
>예시 : 만약 문자열 `s = "abcdef"`이 있다면, `s`의 각 suffixes는 다음과 같다
    - `"abcdef"` (전체 문자열)
    - `"bcdef"`
    - `"cdef"`
    - `"def"`
    - `"ef"`
    - `"f"`
    - 빈 문자열 `""`

문자열이나 시퀀스를 입력값으로 받는 경우 즉, 단어 정렬이나 블랙잭 등 시퀀스를 입력값으로 받는 경우들은 **suffix**를 사용하여 해결할 수 있다.
문자열의 시작점부터 뜯어내면 suffix만 남는다.
하지만 문자열의 끝부터 뜯어내는 **prefix**(suffix의 반대개념)가 더욱 편리한 경우도 존재한다.
**suffix**와 **prefix** 모두 선형 공간$\theta(n)$을 가지고 있으므로 효율적이다.

>**일반적으로** suffix와 prefix 두 개를 같이 사용하는 경우는 없다. suffix와 prefix로도 해결하지 못하는 문제라면 **부분 문자열(substrings)** 을 사용해야 한다.
 
**Substrings**
`모든 i와 j에 대해서 i <= j` `X[i:j]`
공간 복잡도 $\theta(n)$
대부분의 경우에선 suffixes와 prefixes가 더욱 선호된다. 하지만 두 개의 방법으로 해결할 수 없는 경우 substirngs(부분 문자열)을 사용해야 한다. 여전히 다항식으로 표현 가능하고 좋은 방법이다.

**문제 1) 괄호 묶기**
결합법칙이 성립되는 표현이 주어졌을 때 특정 순서대로 계산하길 원한다. 주어진 표현이 n개의 행렬의 곱셈인 경우에서 기본적으로 행렬의 곱셈은 결합법칙이 성립되지 않아 주어진 순서대로 계산을 할 수밖에 없지만, 괄호를 어떻게 배치하느냐에 따라서 곱셈 방식의 비용이 더욱 저렴해지는 경우를 DP를 통해 찾는다. ![[첨부 파일 소스/Image File 1/Pasted image 20240920115141 1.png]]
왼쪽 계산에서 열 x 행의 경우 n사이즈의 정사각형이 나오지만, 오른쪽 계산에서의 행 x 열의 경우 단일 숫자가 나온다. 그러므로 오른쪽과 같은 괄호를 이용한 계산이 더욱 효율적이라고 말할 수 있다. 
>그렇다면 해당 문제에서 하위문제를 풀기위한 어떤 해결법을 추측할 수 있을까 ? (모든 경우의 수를 고려하게 되면 경우의 수가 지수적으로 증가하므로 비효율적)

마지막 곱셈 추측하기.
결국 마지막에는 $(A_0 ... A_{k-1}) \cdot (A_k ... A_{n-1})$ 의 곱셈을 진행할 것이다. 따라서 우리는 두 개의 하위 문제를 가지고 있다. 따라서 1단계, 하위 문제는 $A_i... A_{j-1}$까지의 최적해를 구하는 것이다.

**DP의 과정에 대해 이해가 어렵다면, 더 많은 예시를 보고 코드를 써보는게 도움이 될 것이다.**


**편집 거리 문제**
문서에서 문자열 $x$를 $y$로 교체하는데 드는 최소비용 구하기.
한 단어 대신에 다른 단어가 쓰이는 오류가 발생할 수 있는 비용이 최소인 집합 구하기. 그리고 사전의 모든 단어에 대해 해당 작업을 수행하고 나면, 실제로 입력하고 싶은 단어와 비슷한 형태를 얻을 수 있다. **문서에서 오탈자 찾기**와 유사.

문자열 변형에서 일어날 수 있는 작업
1) insert C (문자 하나 빼먹고 입력했을 경우)
2) delete C (문자를 하나 더 입력했을 경우)
3) replace C -> C' (문자를 바꿔서 입력했을 경우)

편집 거리 문제에 포함되는 또다른 예로는 **최장 공통 부분 시퀀스**(**L**ongest **C**ommon **S**ubsequence)인 **LCS** 문제가 있다.
만일 삽입과 삭제의 비용이 1, 교체의 비용이 0($c = c'$)또는 **무한대**($c != c'$)이라면 이는 **LCS**문제에 속한다.

>**LCS 문제 예시** (두 단어에서 최장 공통 부분 문자를 구하여라)
>H I E R O G L Y P H O L O G Y
>M I C H A E L A N G E L O
>정답은 **H**I**E**ROG**L**YPHO**LO**GY vs MIC**H**A**EL**ANGE**LO** => **HELLO**

**편집 거리 동적 계획법(DP)**
1. **하위 문제**
	하위 문제는 c(i,j)만큼 존재한다. 모든 i와 j에 대하여 각각 $x[i:]$, $y[j:]$개 만큼 있으므로, $\theta(x \cdot y)$만큼의 하위 문제가 존재한다.
2. **추측**
	$x$를 $y$로 바꾸기 위해 **3가지 선택지**가 존재한다.
	- $x[i]$ **삭제**
	- $x$에 $y[i]$ **삽입**
	- $x[i]$를 $y[j]$로 **교체**

3. **반복**
	c(i, j) = 아래 중 최소값
	- $x[i]$를 삭제하는 비용 + $c(i + 1, j) \space$ if $i < \left\vert x \right\vert$
	- $y[j]$를 삽입하는 비용 + $c(i, j + 1)$ if $i < \left\vert y \right\vert$
	- $x[i]$를 $y[j]$로 교체하는 비용 + $c(i + 1, j + 1)$ if $i < \left\vert x \right\vert \space$ & $\space i < \left\vert y \right\vert$

4.  **위상학적 순서**
	DAG를 2차원 표로 나타낼 수 있다.![[첨부 파일 소스/Image File 1/Pasted image 20240920163230 1.png]]
	2중 for문을 진행할 때마다 DP(i, j)는 원점에 가까워질 것이고, 결과적으로 우리가 구하려던 DP(0, 0)의 값을 구할 수 있다.
	해당 간선의 가중치는 각 연산의 비용에 해당한다.

5. **원래 문제**
	 위의 방법으로 DAG의 오른쪽 아래 가장자리부터 왼쪽 위 가장자리까지의 최단 거리를 구하면 DP(0, 0)의 값을 구할 수 있다.
	 또는 그냥 2중 for문을 통해 계산해도 된다. 둘은 같은 방식이다.
	총 실행 시간은 하위 문제의 개수 * 하위 문제의 실행 시간 이므로 하위 문제의 실행 시간은 상수 시간이 걸린다. 그러므로 상수시간 * 하위 문제의 개수(x * y)이므로
	time = $\theta(\left\vert x \right\vert \cdot \left\vert y \right\vert)$의 시간이 걸린다.
	편집 거리 문제에 대한 최선의 알고리즘이다.
	 


### 7.4 동적 프로그래밍 4
#### 강의 노트
![[첨부 파일 소스/PDF File 1/MIT6_006F11_lec22 1.pdf]]
####

**DP의 5단계 외 새로운 개념**
- **2 kinds of guessing(두 번째 종류의 추측)**

## 8. Advanced Topics
### 8.1 계산 복잡도
#### 강의 노트

#####
**P** : 다항식 문제 다항 시간(poly time)안에 풀 수 있는 문제
**NP** : non deterministic P 비결정적 다항식


#### 리덕션
문제를 처음부터 해결하는 대신 이미 해결 방안을 아는 문제로 바꾼다. **가장 보편적인 알고리즘 디자인 기술**
ex)  가중치가 없는 그래프의 최단경로를 BFS대신 다익스트라 코드를 통해 모든 가중치를 1로 두고 해결.


### 8.2 병렬 프로세서 구조 & 알고리즘

동적 프로그래밍에서 `DP[k, p1]` 이런 방식은 풀어야 할 문제를 의미한다.
어떤 방식으로 이루어져있는 것이 아닌 내가 정의하는 것이다.
그리고 이 문제를 토대로 하위 문제를 정의하면 되는 것이다.
Ex) `DP[k, p1]`은 `m1`에서 `mk`까지의 메모리 접근의 prefix에 대한 최적해의 비용이다. 프로그램이 `p1`에서 시작해 `pi`에서 끝나는 경우.

**가정**
모든 계산된 `k`와 `pi`를 가지고 있다고 가정. 몇 개의 하위 문제가 있을까 ? -> 
