활성함수(활성화 함수) = 입력 신호의 총합을 출력 신호로 변환하는 함수

![[Pasted image 20230916193132.png]]
왜 꼭 비선형이어야 하는가 ?
-> 신경망에서 선형함수를 사용하면 신경망의 층을 깊게 만들 의미가 없다
- 왼쪽 시그모이드 함수
- 가운데 하이퍼블릭탄젠트 함수
- 오른쪽 ReLU - 오늘날 가장 많이 쓰는 함수. 선형함수처럼 보이지만 전형적인 비선형 함수이다.
